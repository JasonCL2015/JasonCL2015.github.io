<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[配置中心-Apollo配置使用]]></title>
    <url>%2F2018%2F12%2F19%2F%E7%BB%9F%E4%B8%80%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83-Apollo%E9%85%8D%E7%BD%AE%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[微服务盛行的当下，对于应用的稳定性要求越来越高，随着项目数量激增，各种环境问题逐渐让人头疼，针对这种情况，我们需要一个能够主动发现应用健康问题的监控平台。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>配置中心</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[应用健康检查实现]]></title>
    <url>%2F2018%2F12%2F17%2F%E5%BA%94%E7%94%A8%E5%81%A5%E5%BA%B7%E6%A3%80%E6%9F%A5%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[微服务盛行的当下，对于应用的稳定性要求越来越高，随着项目数量激增，各种环境问题逐渐让人头疼，针对这种情况，我们需要一个能够主动发现应用健康问题的监控平台。 背景随着公司的不断发展，内部项目越来越多，在诸多测试环境中部署运行时，经常碰到环境不通，应用服务没成功响应请求的情况，导致测试进度一再延期，提测期间运维投入精力较大。归根结底，我们对于已部署成功的应用缺乏及时有效全面的健康检查，导致出问题时被动解决。为解决这一困扰，我决定开发一套健康检查监控平台，以达到及时监控各环境的应用运行状况，主动发现环境问题的目的。 设计图 实现健康检查监控平台的实现主要基于Spring Boot Actuator + Micrometer 1 Promethus + Grafana技术栈。以下逐步讲解如何基于这些技术栈搭建一套应用健康检查监控平台。因为SpringBoot在2.0中已经集成Micrometer，对我们输出Promethus规范的数据帮助很大，但本文是基于SpringBoot1.5.x的实现，所以有必要提一下Micrometer。 应用健康数据搜集–Spring Boot Actuator Spring Boot 提供了很多可选功能帮助我们在生产环境下监控应用健康状态，可使用HTTP endpoints，JMX甚至SSH方式访问。为兼容历史Spring MVC应用，本文采用HTTP endpoints访问方式进行数据采集。 首先引入actuator依赖1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;&lt;/dependency&gt; or123dependencies &#123; compile("org.springframework.boot:spring-boot-starter-actuator")&#125; 加入以上依赖后，我们会发现应用在启动日志中会出现很多mapping,例如/health,/info,/metrics等，这些是actuator提供的endpoints。我们这里需要用到的是/prometheus，一个将metrics信息以prometheus能够解析的风格暴露出来的路由。为了端点访问安全起见，我们需要给enpoints加上security验证，通知关闭不需要的endpoints。配置如下：12345endpoints.enabled=falseendpoints.prometheus.enabled=truemanagement.security.enabled=truesecurity.user.name=adminsecurity.user.password=admin 这样我们即可通过访问地址http://ip:端口/promethues 得到我们想要的健康监控数据。 自定义健康检查一般情况下，一个业务应用需要检查的健康维度主要有DB，缓存，RPC连接，消息服务等。在SpringBoot Actuator中已经给我们提供了DataSource，Redis的健康检查实现，如果用到Dubbo这类RPC框架，阿里的dubbo-spring-boot-actuator也提供了Dubbo的健康检查。如果这些依然不能满足需要，可以自定义健康检查实现，只要继承AbstractHealthIndicator，实现doHealthCheck(Builder builder)方法即可。例如以下实现RocketMQ连接检查123456789101112131415161718192021222324252627@Override protected void doHealthCheck(Health.Builder builder) &#123; String mqNameServerAddress = mhcContext.getEnvironment().getProperty(MQ_NAMESERVER_ADDRESS); if (StringUtils.isEmpty(mqNameServerAddress)) &#123; logger.warn("Camaro MQ name-server-address is undefined, please check your configure."); builder.unknown(); &#125; else &#123; try &#123; if (mqProducer == null) &#123; mqProducer = new DefaultMQProducer(MQ_PRODUCE_GROUP); mqProducer.setNamesrvAddr(mqNameServerAddress); mqProducer.start(); &#125; Message msg = new Message(MQ_PRODUCE_TOPIC, ("Hello RocketMQ").getBytes()); SendResult sendResult = mqProducer.send(msg); if (sendResult.getSendStatus().equals(SendStatus.SEND_OK)) &#123; builder.up(); &#125; else &#123; builder.down().withDetail("send_status", sendResult.getSendStatus()); &#125; &#125; catch (Exception e) &#123; logger.error(e.toString()); builder.down(e); &#125; &#125; &#125; 健康检查数据转换–MicrometerMicrometer提供了很多支持主流监控框架数据格式转换的接口，当然也支持Prometheus，我们这里引入Micirometer就是这个目的。详情可参考官方文档这里介绍如何自定义metircs数据转换，首先引入三方依赖1234567891011&lt;dependency&gt; &lt;groupId&gt;io.micrometer&lt;/groupId&gt; &lt;artifactId&gt;micrometer-registry-prometheus&lt;/artifactId&gt; &lt;version&gt;LATEST&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;io.micrometer&lt;/groupId&gt; &lt;artifactId&gt;micrometer-spring-legacy&lt;/artifactId&gt; &lt;version&gt;LATEST&lt;/version&gt;&lt;/dependency&gt; 源码中发现metrics数据采集都是注册到MeterRegistry中，我们可以实现MeterBinder接口，重写bindTo(MeterRegistry registry)方法，将metrics数据注册到MeterRegistry，也可以用Metrics.globalRegistry获取一个全局注册对象，将metrics数据注册进去。例如以下实现Dubbo Filter接口对provider服务监控调用计数和耗时123456789101112131415161718@Activate(group = Constants.PROVIDER)public class ProviderInvokeStaticsFilter implements Filter &#123; private MeterRegistry metrics = Metrics.globalRegistry; @SuppressWarnings(&quot;Duplicates&quot;) @Override public Result invoke(Invoker&lt;?&gt; invoker, Invocation invocation) throws RpcException &#123; String key = &quot;dubbo_provider&quot;; String url = invoker.getInterface().getCanonicalName() + &quot;.&quot; + invocation.getMethodName(); Timer.Sample sample = Timer.start(metrics); try &#123; return invoker.invoke(invocation); &#125; finally &#123; sample.stop(metrics.timer(key, &quot;url&quot;, url)); metrics.counter(key + &quot;.count&quot;, &quot;url&quot;, url).increment(); &#125; &#125;&#125; 实现MeterBinder接口的示例网上很多范例，这里不再列出。Micrometer遵循Prometheus的Metrics类型Counter、Gagues、Sunmary等。 对接监控框架—Prometheus上面的准备工作已经全部做完，为了对接Prometheus监控框架，需遵循它的数据格式，我们通过Micrometer将actuator采集的应用数据转换为该格式。那现在要做的就是部署Promethes，先对它做个简单介绍，详情见官网文档,或者中文promethes-book。Prometheus作为新一代的云原生监控系统，目前已经有超过650+位贡献者参与到Prometheus的研发工作上，并且超过120+项的第三方集成。它具备强大的数据模型，易于管理，易于集成，高效，可扩展等特征，架构原理图如下从图中不难发现Prometheus server通过Jobs exporters，Pushgateway或service discovery的方式进行pull metrics并持久化，提供强大的PromQL查询语法给展示层，同时可配置告警规则，将告警信息push到Alertmanager，Alertmanager负责集成各种notify方式。结构很清晰，各组件分工很明确。首先去下载prometheus,alertmanager，解压后进入根目录后执行命令，启动prometheus，这里加入–web.enable-lifecycle是为了热加载配置文件prometheus.yml。1./prometheus --config.file=prometheus.yml --web.enable-lifecycle 热加载方式推荐(下面IP和端口号为prometheus启动的服务IP和端口)1curl -X POST http://127.0.0.1:9090/-/reload 启动完成后，重点是配置prometheus.yml，看示例12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152# my global configglobal: scrape_interval: 15s # Set the scrape interval to every 15 seconds. Default is every 1 minute. evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute. # scrape_timeout is set to the global default (10s).# Alertmanager configurationalerting: alertmanagers: - static_configs: - targets: - 127.0.0.1:9093# Load rules once and periodically evaluate them according to the global 'evaluation_interval'.rule_files: - "*_alertrules.yml" # - "malibu_test_rules.yml" # - "second_rules.yml"# A scrape configuration containing exactly one endpoint to scrape:# Here it's Prometheus itself.scrape_configs: # The job name is added as a label `job=&lt;job_name&gt;` to any timeseries scraped from this config. # - job_name: 'node' # static_configs: # - targets: ['localhost:9100'] - job_name: 'server-malibu' metrics_path: /mhc_health/prometheus basic_auth: username: admin password: admin static_configs: - targets: ['127.0.0.1:8086'] # - job_name: 'server-rank' # metrics_path: /mhc_health/prometheus # static_configs: # - targets: ['localhost:8086'] # - job_name: consul_node # metrics_path: /mhc_health/prometheus # scheme: http # consul_sd_configs: # - server: 172.21.10.49:8500 # relabel_configs: # - source_labels: ["__meta_consul_service_id"] # target_label: "service_id" # - source_labels: ["__meta_consul_service_id"] # regex: "landrover-(.+)|consul" # action: drop # - source_labels: ["__meta_consul_tags"] # target_label: "env" global 顾名思义为全局定义，这里配置全局扫描频率，超时时间，外部系统标签等alerting 配置告警管理模块，这里包括重写标签配置和告警管理配置rule_files 配置告警规则扫描文件，可模糊匹配scrape_configs 扫描job配置，这里可用静态扫描方式static_configs或者动态发现配置如consul_sd_configs,serverset_sd_configs等，如上面示例配置consul_sd_configs的server以及重写标签。 对接告警模块和通知方式配置–alertmanager,webhook首先启动告警模块服务1./alertmanager --config.file=simple.yml 告警模块主要关注Grouping（告警分组）和Inhibition（告警抑制）,以示例说明12345678910111213141516171819202122232425262728293031323334353637global: resolve_timeout: 2mroute: receiver: 'send_to_dingtalk_ops' group_by: ['opsgroup', 'ownergroup'] #等待10s内的告警信息，合并后一起发送 group_wait: 10s #发送告警信息的间隔10s group_interval: 10s repeat_interval: 3m routes: - receiver: 'send_to_dingtalk_owner' group_by: ['ownergroup'] match: team: dev - receiver: 'send_to_dingtalk_ops' group_by: ['opsgroup'] match: team: opsreceivers:- name: 'send_to_dingtalk_ops' webhook_configs: - send_resolved: false url: 'http://localhost:8060/dingtalk/webhook1/send'- name: 'send_to_dingtalk_owner' webhook_configs: - send_resolved: false url: 'http://localhost:8060/dingtalk/webhook2/send'inhibit_rules: - source_match: server_status: ss target_match_re: team: '^[a-z]+$' equal: ['job', 'instance'] global 定义全局变量参数，如smtp配置，hook配置等，这里注意resovle_timeout,该参数定义了当Alertmanager持续多长时间未接收到告警后标记告警状态为resolved（已解决）。该参数的定义可能会影响到告警恢复通知的接收时间，读者可根据自己的实际场景进行定义，其默认值为5分钟。route 告警路由配置，这里定义告警接收方，也可以基于标签配置路由规则，如上面示例，根据标签team匹配不同消息的接收方。receivers 接收方配置，这里支持email,slack,hipchat,webhook等方式。我们这里对接钉钉机器人，使用webhook方式进行消息发送。send_resolved表示是否发送告警问题已解决消息。inhibit_rules 抑制机制，可以避免当某种问题告警产生之后用户接收到大量由此问题导致的一系列的其它告警通知。例如当集群不可用时，用户可能只希望接收到一条告警，告诉他这时候集群出现了问题，而不是大量的如集群中的应用异常、中间件服务异常的告警通知。抑制规则通过source_match,target_match和equal匹配符合条件的其它告警信息（target_match），如果equal中的标签值一致，则不再发送消息。钉钉消息 alertmanager给webhook传输的原始数据结构并非钉钉官方的markdown消息通知数据结构，所以我们需要中间做一次数据映射转换，社区已经有位大牛写好了这块，详情使用方式请参见 Prometheus AlertManager WebHook For DingTalk 监控数据展现–Grafana前面做好了数据的采集、转换、对接监控平台和告警管理，接下来要做一些监控数据的展现，这里推荐Grafana可视化套件，它是一个开源的度量分析工具，支持很多开源数据源，其中就包括Prometheus，这里不多介绍Grafana，使用方法可见官网，值得注意的是官方提供了很多优秀的Dashboards，可以直接拿来使用，详见链接 https://grafana.com/dashboards ,不过更多高级的图表需要根据你对接的数据源所支持的SQL来实现，例如PromQL。]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>健康检查</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TCP与UDP详解]]></title>
    <url>%2F2018%2F11%2F15%2FTCP%E4%B8%8EUDP%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[详细拆解传输层通信协议TCP和UDP。 TCPTCP，英文全称Transmission Control Protocol，传输控制协议，是一种面向连接的、可靠的、基于字节流的传输层通信协议，由IETF的RFC 793定义。在因特网协议族中，TCP层是位于IP层之上，应用层之下的中间层。应用层向TCP层发送用于网间传输的、用8位字节表示的数据流，然后TCP把数据流分割成适当长度的报文段，之后TCP把结果包传给IP层，由它来通过网络将包传送给接收端实体的TCP层。TCP为了保证不发生丢包，就给每个包一个序号，同时序号也保证了传送到接收端实体的包的按序接收。然后接收端实体对已成功收到的包发回一个相应的确认（ACK），如果发送端实体在合理的往返时延（RTT）内未收到确认，那么对应的数据包就被假设为已丢失将会被进行重传。TCP用一个校验和函数来检验数据是否有错误，在发送和接收时都要计算校验和。 运作方式TCP协议的运行可划分为三个阶段：连接创建(connection establishment)、数据传送（data transfer）和连接终止（connection termination）。操作系统将TCP连接抽象为套接字表示的本地端点（local end-point），作为编程接口给程序使用。在TCP连接的生命期内，本地端点要经历一系列的状态改变。 连接创建（connection establishment）TCP用三路握手（或称三次握手，three-way handshake）过程创建一个连接。在连接创建过程中，很多参数要被初始化，例如序号被初始化以保证按序传输和连接的强壮性。TCP连接的正常创建一对终端同时初始化一个它们之间的连接是可能的。但通常是由一端打开一个套接字（socket）然后监听来自另一方的连接，这就是通常所指的被动打开（passive open）。服务器端被被动打开以后，用户端就能开始创建主动打开（active open）。 客户端通过向服务器端发送一个SYN来创建一个主动打开，作为三路握手的一部分。客户端把这段连接的序号设定为随机数A。 服务器端应当为一个合法的SYN回送一个SYN/ACK。ACK的确认码应为A+1，SYN/ACK包本身又有一个随机产生的序号B。 最后，客户端再发送一个ACK。当服务端收到这个ACK的时候，就完成了三路握手，并进入了连接创建状态。此时包的序号被设定为收到的确认号A+1，而响应号则为B+1。如果服务器端接到了客户端发的SYN后回了SYN-ACK后客户端掉线了，服务器端没有收到客户端回来的ACK，那么，这个连接处于一个中间状态，即没成功，也没失败。于是，服务器端如果在一定时间内没有收到的TCP会重发SYN-ACK。在Linux下，默认重试次数为5次，重试的间隔时间从1s开始每次都翻倍，5次的重试时间间隔为1s, 2s, 4s, 8s, 16s，总共31s，第5次发出后还要等32s才知道第5次也超时了，所以，总共需要 1s + 2s + 4s+ 8s+ 16s + 32s = 63s，TCP才会断开这个连接。使用三个TCP参数来调整行为：tcp_synack_retries 减少重试次数；tcp_max_syn_backlog，增大SYN连接数；tcp_abort_on_overflow决定超出能力时的行为。数据传输（data transfer）在TCP的数据传送状态，很多重要的机制保证了TCP的可靠性和强壮性。它们包括：使用序号，对收到的TCP报文段进行排序以及检测重复的数据；使用校验和检测报文段的错误，即无错传输[3]；使用确认和计时器来检测和纠正丢包或延时；流控制（Flow control）；拥塞控制（Congestion control）；丢失包的重传。 可靠传输通常在每个TCP报文段中都有一对序号和确认号。TCP报文发送者称自己的字节流的编号为序号，称接收到对方的字节流编号为确认号。TCP报文的接收者为了确保可靠性，在接收到一定数量的连续字节流后才发送确认。这是对TCP的一种扩展，称为选择确认（Selective Acknowledgement）。选择确认使得TCP接收者可以对乱序到达的数据块进行确认。每一个字节传输过后，ISN号都会递增1。 通过使用序号和确认号，TCP层可以把收到的报文段中的字节按正确的顺序交付给应用层。序号是32位的无符号数，在它增大到232-1时，便会回绕到0。对于ISN的选择是TCP中关键的一个操作，它可以确保强壮性和安全性。 TCP协议使用序号（sequence number）标识每端发出的字节的顺序，从而另一端接收数据时可以重建顺序，无惧传输时的包的乱序交付或丢包。在发送第一个包时（SYN包），选择一个随机数作为序号的初值，以克制TCP序号预测攻击. 发送确认包(Acks)，携带了接收到的对方发来的字节流的编号，称为确认号，以告诉对方已经成功接收的数据流的字节位置。Ack并不意味着数据已经交付了上层应用程序。 可靠性通过发送方检测到丢失的传输数据并重传这些数据。包括超时重传（Retransmission timeout，RTO）与重复累计确认（duplicate cumulative acknowledgements，DupAcks）。 基于重复累计确认的重传如果一个包（不妨设它的序号是100，即该包始于第100字节）丢失，接收方就不能确认这个包及其以后的包，因为采用了累计ack。接收方在收到100以后的包时，发出对包含第99字节的包的确认。这种重复确认是包丢失的信号。发送方如果收到3次对同一个包的确认，就重传最后一个未被确认的包。阈值设为3被证实可以减少乱序包导致的无作用的重传（spurious retransmission）现象。选择性确认(SACK)的使用能明确反馈哪个包收到了，极大改善了TCP重传必要的包的能力。 超时重传发送方使用一个保守估计的时间作为收到数据包的确认的超时上限。如果超过这个上限仍未收到确认包，发送方将重传这个数据包。每当发送方收到确认包后，会重置这个重传定时器。典型地，定时器的值设定为 smoothed RTT+max(G,4*RTT variation)G是时钟粒度。进一步，如果重传定时器被触发，仍然没有收到确认包，定时器的值将被设为前次值的二倍（直到特定阈值）。这可对抗 中间人攻击方式的拒绝服务攻击，这种攻击愚弄发送者重传很多次导致接受者被压垮。 校验和TCP的16位的校验和（checksum）的计算和检验过程如下：发送者将TCP报文段的头部和数据部分的和计算出来，再对其求反码（一的补数），就得到了校验和，然后将结果装入报文中传输。（这里用反码和的原因是这种方法的循环进位使校验和可以在16位、32位、64位等情况下的计算结果再叠加后相同）接收者在收到报文后再按相同的算法计算一次校验和。这里使用的反码使得接收者不用再将校验和字段保存起来后清零，而可以直接将报文段连同校验加总。如果计算结果是全部为一，那么就表示了报文的完整性和正确性。 注意：TCP校验和也包括了96位的伪头部，其中有源地址、目的地址、协议以及TCP的长度。这可以避免报文被错误地路由。 按现在的标准，TCP的校验和是一个比较脆弱的校验。出错概率高的数据链路层需要更高的能力来探测和纠正连接错误。TCP如果是在今天设计的，它很可能有一个32位的CRC校验来纠错，而不是使用校验和。但是通过在第二层使用通常的CRC校验或更完全一点的校验可以部分地弥补这种脆弱的校验。第二层是在TCP层和IP层之下的，比如PPP或以太网，它们使用了这些校验。但是这也并不意味着TCP的16位校验和是冗余的，对于因特网传输的观察，表明在受CRC校验保护的各跳之间，软件和硬件的错误通常也会在报文中引入错误，而端到端的TCP校验能够捕捉到大部分简单的错误。这就是应用中的端到端原则。 流量控制流量控制用来避免主机分组发送得过快而使接收方来不及完全收下，一般由接收方通告给发送方进行调控。 TCP使用滑动窗口协议实现流量控制。接收方在“接收窗口”域指出还可接收的字节数量。发送方在没有新的确认包的情况下至多发送“接收窗口”允许的字节数量。接收方可修改“接收窗口”的值。 TCP包的序号与接收窗口的行为很像时钟。当接收方宣布接收窗口的值为0，发送方停止进一步发送数据，开始了“保持定时器”（persist timer），以避免因随后的修改接收窗口的数据包丢失使连接的双侧进入死锁，发送方无法发出数据直至收到接收方修改窗口的指示。当“保持定时器”到期时，TCP发送方尝试恢复发送一个小的ZWP包（Zero Window Probe），期待接收方回复一个带着新的接收窗口大小的确认包。一般ZWP包会设置成3次，如果3次过后还是0的话，有的TCP实现就会发RST把链接断了。 如果接收方以很小的增量来处理到来的数据，它会发布一系列小的接收窗口。这被称作愚蠢窗口综合症，因为它在TCP的数据包中发送很少的一些字节，相对于TCP包头是很大的开销。解决这个问题，就要避免对小的window size做出响应，直到有足够大的window size再响应： 接收端使用David D Clark算法：如果收到的数据导致window size小于某个值，可以直接ack把window给关闭了，阻止了发送端再发数据。等到接收端处理了一些数据后windows size大于等于了MSS，或者接收端buffer有一半为空，就可以把window打开让发送端再发数据过来。发送端使用Nagle算法来延时处理，条件一：Window Size&gt;=MSS 或是 Data Size &gt;=MSS；条件二：等待时间或是超时200ms，这两个条件有一个满足，才会发数据，否则就是在积累数据。Nagle算法默认是打开的，所以对于一些需要小包场景的程序——比如像telnet或ssh这样的交互性程序，需要关闭这个算法。可以在Socket设置TCP_NODELAY选项来关闭这个算法。 拥塞控制拥塞控制是发送方根据网络的承载情况控制分组的发送量，以获取高性能又能避免拥塞崩溃（congestion collapse，网络性能下降几个数量级）。这在网络流之间产生近似最大最小公平分配。 发送方与接收方根据确认包或者包丢失的情况，以及定时器，估计网络拥塞情况，从而修改数据流的行为，这称为拥塞控制或网络拥塞避免。 TCP的现代实现包含四种相互影响的拥塞控制算法：慢开始、拥塞避免、快速重传、快速恢复。 此外，发送方采取“超时重传”（retransmission timeout，RTO），这是估计出来回通信延迟 (RTT) 以及RTT的方差。 RFC793中定义的计算SRTT的经典算法：指数加权移动平均（Exponential weighted moving average） 先采样RTT，记下最近好几次的RTT值。做平滑计算SRTT公式为： SRTT=( α SRTT)+((1- α )RTT)，其中 α 取值在0.8 到 0.9之间计算RTO，公式： RTO=min(UBOUND,max(LBOUND,(β *SRTT))，其中 UBOUND是最大的timeout时间上限值，LBOUND是最小的timeout时间下限值，β值一般在1.3到2.0之间。1987年，出现计算RTT的Karn算法或TCP时间戳（RFC 1323），最大特点是——忽略重传，不把重传的RTT做采样。但是，如果在某一时间，网络闪动，突然变慢了，产生了比较大的延时，这个延时导致要重转所有的包（因为之前的RTO很小），于是，因为重转的不算，所以，RTO就不会被更新，这是一个灾难。为此，Karn算法一发生重传，就对现有的RTO值翻倍。这就是的Exponential backoff。 1988年，在RFC 6298中给出范·雅各布森算法取平均以获得平滑往返时延（Smoothed Round Trip Time，SRTT），作为最终的RTT估计值。这个算法在被用在今天的TCP协议中： SRTT=SRTT+ α *(RTT-SRTT) DevRTT=(1-β )DevRTT+β |RTT-SRTT| RTO= μ SRTT+∂ DevRTT 其中：DevRTT是Deviation RTT。在Linux下，α = 0.125，β = 0.25， μ = 1，∂= 4 当前有很多TCP拥塞控制算法在研究中。 连接终止（connection termination）连接终止使用了四路握手过程（或称四次握手，four-way handshake），在这个过程中连接的每一侧都独立地被终止。当一个端点要停止它这一侧的连接，就向对侧发送FIN，对侧回复ACK表示确认。因此，拆掉一侧的连接过程需要一对FIN和ACK，分别由两侧端点发出。 首先发出FIN的一侧，如果给对侧的FIN响应了ACK，那么就会超时等待2*MSL时间，然后关闭连接。在这段超时等待时间内，本地的端口不能被新连接使用；避免延时的包的到达与随后的新连接相混淆。RFC793定义了MSL为2分钟，Linux设置成了30s。参数tcp_max_tw_buckets控制并发的TIME_WAIT的数量，默认值是180000，如果超限，那么，系统会把多的TIME_WAIT状态的连接给destory掉，然后在日志里打一个警告（如：time wait bucket table overflow） 连接可以工作在TCP半开状态。即一侧关闭了连接，不再发送数据；但另一侧没有关闭连接，仍可以发送数据。已关闭的一侧仍然应接收数据，直至对侧也关闭了连接。 也可以通过测三次握手关闭连接。主机A发出FIN，主机B回复FIN &amp; ACK，然后主机A回复ACK. 一些主机（如Linux或HP-UX）的TCP栈能实现半双工关闭序列。这种主机如果主动关闭一个连接但还没有读完从这个连接已经收到的数据，该主机发送RST代替FIN。这使得一个TCP应用程序能确认远程应用程序已经读了所有已发送数据，并等待远程侧发出的FIN。但是远程的TCP栈不能区分Connection Aborting RST与Data Loss RST，两种原因都会导致远程的TCP栈失去所有的收到数据。 一些应用协议使用TCP open/close handshaking，因为应用协议的TCP open/close handshaking可以发现主动关闭的RST问题。 UDPUDP，用户数据包协议，英文全称User Datagram Protocol，是一个简单的面向数据包的传输层协议，该协议由David P.Reed在1980年设计且在RFC 768中被规范。在TCP/IP模型中，UDP为网络层以上和应用层以下提供了一个简单的接口。UDP只提供数据的不可靠传递，它一旦把应用程序发给网络层的数据发送出去，就不保留数据备份（所以UDP有时候也被认为是不可靠的数据包协议）。UDP在IP数据包的头部仅仅加入了复用和数据校验（字段）。 UDP首部字段由4个部分组成，其中两个是可选的。各16bit的来源端口和目的端口用来标记发送和接受的应用进程。因为UDP不需要应答，所以来源端口是可选的，如果来源端口不用，那么置为零。在目的端口后面是长度固定的以字节为单位的长度域，用来指定UDP数据报包括数据部分的长度，长度最小值为8byte。首部剩下地16bit是用来对首部和数据部分一起做校验和（Checksum）的，这部分是可选的，但在实际应用中一般都使用这一功能。 由于缺乏可靠性且属于非连接导向协议，UDP应用一般必须允许一定量的丢包、出错和复制粘贴。但有些应用，比如TFTP，如果需要则必须在应用层增加根本的可靠机制。但是绝大多数UDP应用都不需要可靠机制，甚至可能因为引入可靠机制而降低性能。流媒体（流技术）、即时多媒体游戏和IP电话（VoIP）一定就是典型的UDP应用。如果某个应用需要很高的可靠性，那么可以用传输控制协议（TCP协议）来代替UDP。 由于缺乏拥塞控制（congestion control），需要基于网络的机制来减少因失控和高速UDP流量负荷而导致的拥塞崩溃效应。换句话说，因为UDP发送者不能够检测拥塞，所以像使用包队列和丢弃技术的路由器这样的网络基本设备往往就成为降低UDP过大通信量的有效工具。数据报拥塞控制协议（DCCP）设计成通过在诸如流媒体类型的高速率UDP流中，增加主机拥塞控制，来减小这个潜在的问题。 典型网络上的众多使用UDP协议的关键应用一定程度上是相似的。这些应用包括域名系统（DNS）、简单网络管理协议（SNMP）、动态主机配置协议（DHCP）、路由信息协议（RIP）和某些影音流服务等等。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTTP状态码梳理]]></title>
    <url>%2F2018%2F11%2F13%2FHTTP%E7%8A%B6%E6%80%81%E7%A0%81%E6%A2%B3%E7%90%86%2F</url>
    <content type="text"><![CDATA[HTTP状态码详解 状态码 含义 常见解决方法 100 客户端应当继续发送请求。这个临时响应是用来通知客户端它的部分请求已经被服务器接收，且仍未被拒绝。客户端应当继续发送请求的剩余部分，或者如果请求已经完成，忽略这个响应。服务器必须在请求完成后向客户端发送一个最终响应。 101 服务器已经理解了客户端的请求，并将通过Upgrade 消息头通知客户端采用不同的协议来完成这个请求。在发送完这个响应最后的空行后，服务器将会切换到在Upgrade 消息头中定义的那些协议。 只有在切换新的协议更有好处的时候才应该采取类似措施。例如，切换到新的HTTP 版本比旧版本更有优势，或者切换到一个实时且同步的协议以传送利用此类特性的资源。 102 由WebDAV（RFC 2518）扩展的状态码，代表处理将被继续执行。 200 请求已成功，请求所希望的响应头或数据体将随此响应返回。 201 请求已经被实现，而且有一个新的资源已经依据请求的需要而建立，且其 URI 已经随Location 头信息返回。假如需要的资源无法及时建立的话，应当返回 ‘202 Accepted’。 202 服务器已接受请求，但尚未处理。正如它可能被拒绝一样，最终该请求可能会也可能不会被执行。在异步操作的场合下，没有比发送这个状态码更方便的做法了。 返回202状态码的响应的目的是允许服务器接受其他过程的请求（例如某个每天只执行一次的基于批处理的操作），而不必让客户端一直保持与服务器的连接直到批处理操作全部完成。在接受请求处理并返回202状态码的响应应当在返回的实体中包含一些指示处理当前状态的信息，以及指向处理状态监视器或状态预测的指针，以便用户能够估计操作是否已经完成。 203 服务器已成功处理了请求，但返回的实体头部元信息不是在原始服务器上有效的确定集合，而是来自本地或者第三方的拷贝。当前的信息可能是原始版本的子集或者超集。例如，包含资源的元数据可能导致原始服务器知道元信息的超级。使用此状态码不是必须的，而且只有在响应不使用此状态码便会返回200 OK的情况下才是合适的。 204 服务器成功处理了请求，但不需要返回任何实体内容，并且希望返回更新了的元信息。响应可能通过实体头部的形式，返回新的或更新后的元信息。如果存在这些头部信息，则应当与所请求的变量相呼应。 如果客户端是浏览器的话，那么用户浏览器应保留发送了该请求的页面，而不产生任何文档视图上的变化，即使按照规范新的或更新后的元信息应当被应用到用户浏览器活动视图中的文档。 由于204响应被禁止包含任何消息体，因此它始终以消息头后的第一个空行结尾。 205 服务器成功处理了请求，且没有返回任何内容。但是与204响应不同，返回此状态码的响应要求请求者重置文档视图。该响应主要是被用于接受用户输入后，立即重置表单，以便用户能够轻松地开始另一次输入。 与204响应一样，该响应也被禁止包含任何消息体，且以消息头后的第一个空行结束。 206 服务器已经成功处理了部分 GET 请求。类似于 FlashGet 或者迅雷这类的 HTTP 下载工具都是使用此类响应实现断点续传或者将一个大文档分解为多个下载段同时下载。 该请求必须包含 Range 头信息来指示客户端希望得到的内容范围，并且可能包含 If-Range 来作为请求条件。 响应必须包含如下的头部域： Content-Range 用以指示本次响应中返回的内容的范围；如果是 Content-Type 为 multipart/byteranges 的多段下载，则每一 multipart 段中都应包含 Content-Range 域用以指示本段的内容范围。假如响应中包含 Content-Length，那么它的数值必须匹配它返回的内容范围的真实字节数。 Date ETag 和/或 Content-Location，假如同样的请求本应该返回200响应。 Expires, Cache-Control，和/或 Vary，假如其值可能与之前相同变量的其他响应对应的值不同的话。 假如本响应请求使用了 If-Range 强缓存验证，那么本次响应不应该包含其他实体头；假如本响应的请求使用了 If-Range 弱缓存验证，那么本次响应禁止包含其他实体头；这避免了缓存的实体内容和更新了的实体头信息之间的不一致。否则，本响应就应当包含所有本应该返回200响应中应当返回的所有实体头部域。 假如 ETag 或 Last-Modified 头部不能精确匹配的话，则客户端缓存应禁止将206响应返回的内容与之前任何缓存过的内容组合在一起。 任何不支持 Range 以及 Content-Range 头的缓存都禁止缓存206响应返回的内容。 207 由WebDAV(RFC 2518)扩展的状态码，代表之后的消息体将是一个XML消息，并且可能依照之前子请求数量的不同，包含一系列独立的响应代码。 300 被请求的资源有一系列可供选择的回馈信息，每个都有自己特定的地址和浏览器驱动的商议信息。用户或浏览器能够自行选择一个首选的地址进行重定向。 除非这是一个 HEAD 请求，否则该响应应当包括一个资源特性及地址的列表的实体，以便用户或浏览器从中选择最合适的重定向地址。这个实体的格式由 Content-Type 定义的格式所决定。浏览器可能根据响应的格式以及浏览器自身能力，自动作出最合适的选择。当然，RFC 2616规范并没有规定这样的自动选择该如何进行。 如果服务器本身已经有了首选的回馈选择，那么在 Location 中应当指明这个回馈的 URI；浏览器可能会将这个 Location 值作为自动重定向的地址。此外，除非额外指定，否则这个响应也是可缓存的。 301 被请求的资源已永久移动到新位置，并且将来任何对此资源的引用都应该使用本响应返回的若干个 URI 之一。如果可能，拥有链接编辑功能的客户端应当自动把请求的地址修改为从服务器反馈回来的地址。除非额外指定，否则这个响应也是可缓存的。 新的永久性的 URI 应当在响应的 Location 域中返回。除非这是一个 HEAD 请求，否则响应的实体中应当包含指向新的 URI 的超链接及简短说明。 如果这不是一个 GET 或者 HEAD 请求，因此浏览器禁止自动进行重定向，除非得到用户的确认，因为请求的条件可能因此发生变化。 注意：对于某些使用 HTTP/1.0 协议的浏览器，当它们发送的 POST 请求得到了一个301响应的话，接下来的重定向请求将会变成 GET 方式。 302 请求的资源现在临时从不同的 URI 响应请求。由于这样的重定向是临时的，客户端应当继续向原有地址发送以后的请求。只有在Cache-Control或Expires中进行了指定的情况下，这个响应才是可缓存的。 新的临时性的 URI 应当在响应的 Location 域中返回。除非这是一个 HEAD 请求，否则响应的实体中应当包含指向新的 URI 的超链接及简短说明。 如果这不是一个 GET 或者 HEAD 请求，那么浏览器禁止自动进行重定向，除非得到用户的确认，因为请求的条件可能因此发生变化。 注意：虽然RFC 1945和RFC 2068规范不允许客户端在重定向时改变请求的方法，但是很多现存的浏览器将302响应视作为303响应，并且使用 GET 方式访问在 Location 中规定的 URI，而无视原先请求的方法。状态码303和307被添加了进来，用以明确服务器期待客户端进行何种反应。 303 对应当前请求的响应可以在另一个 URI 上被找到，而且客户端应当采用 GET 的方式访问那个资源。这个方法的存在主要是为了允许由脚本激活的POST请求输出重定向到一个新的资源。这个新的 URI 不是原始资源的替代引用。同时，303响应禁止被缓存。当然，第二个请求（重定向）可能被缓存。 新的 URI 应当在响应的 Location 域中返回。除非这是一个 HEAD 请求，否则响应的实体中应当包含指向新的 URI 的超链接及简短说明。 注意：许多 HTTP/1.1 版以前的 浏览器不能正确理解303状态。如果需要考虑与这些浏览器之间的互动，302状态码应该可以胜任，因为大多数的浏览器处理302响应时的方式恰恰就是上述规范要求客户端处理303响应时应当做的。 304 如果客户端发送了一个带条件的 GET 请求且该请求已被允许，而文档的内容（自上次访问以来或者根据请求的条件）并没有改变，则服务器应当返回这个状态码。304响应禁止包含消息体，因此始终以消息头后的第一个空行结尾。 该响应必须包含以下的头信息： Date，除非这个服务器没有时钟。假如没有时钟的服务器也遵守这些规则，那么代理服务器以及客户端可以自行将 Date 字段添加到接收到的响应头中去（正如RFC 2068中规定的一样），缓存机制将会正常工作。 ETag 和/或 Content-Location，假如同样的请求本应返回200响应。 Expires, Cache-Control，和/或Vary，假如其值可能与之前相同变量的其他响应对应的值不同的话。 假如本响应请求使用了强缓存验证，那么本次响应不应该包含其他实体头；否则（例如，某个带条件的 GET 请求使用了弱缓存验证），本次响应禁止包含其他实体头；这避免了缓存了的实体内容和更新了的实体头信息之间的不一致。 假如某个304响应指明了当前某个实体没有缓存，那么缓存系统必须忽视这个响应，并且重复发送不包含限制条件的请求。 假如接收到一个要求更新某个缓存条目的304响应，那么缓存系统必须更新整个条目以反映所有在响应中被更新的字段的值。 305 被请求的资源必须通过指定的代理才能被访问。Location 域中将给出指定的代理所在的 URI 信息，接收者需要重复发送一个单独的请求，通过这个代理才能访问相应资源。只有原始服务器才能建立305响应。 注意：RFC 2068中没有明确305响应是为了重定向一个单独的请求，而且只能被原始服务器建立。忽视这些限制可能导致严重的安全后果。 306 在最新版的规范中，306状态码已经不再被使用。 307 请求的资源现在临时从不同的URI 响应请求。由于这样的重定向是临时的，客户端应当继续向原有地址发送以后的请求。只有在Cache-Control或Expires中进行了指定的情况下，这个响应才是可缓存的。 新的临时性的URI 应当在响应的 Location 域中返回。除非这是一个HEAD 请求，否则响应的实体中应当包含指向新的URI 的超链接及简短说明。因为部分浏览器不能识别307响应，因此需要添加上述必要信息以便用户能够理解并向新的 URI 发出访问请求。 如果这不是一个GET 或者 HEAD 请求，那么浏览器禁止自动进行重定向，除非得到用户的确认，因为请求的条件可能因此发生变化。 400 1、语义有误，当前请求无法被服务器理解。除非进行修改，否则客户端不应该重复提交这个请求。 2、请求参数有误。 401 当前请求需要用户验证。该响应必须包含一个适用于被请求资源的 WWW-Authenticate 信息头用以询问用户信息。客户端可以重复提交一个包含恰当的 Authorization 头信息的请求。如果当前请求已经包含了 Authorization 证书，那么401响应代表着服务器验证已经拒绝了那些证书。如果401响应包含了与前一个响应相同的身份验证询问，且浏览器已经至少尝试了一次验证，那么浏览器应当向用户展示响应中包含的实体信息，因为这个实体信息中可能包含了相关诊断信息。参见RFC 2617。 402 该状态码是为了将来可能的需求而预留的。 403 服务器已经理解请求，但是拒绝执行它。与401响应不同的是，身份验证并不能提供任何帮助，而且这个请求也不应该被重复提交。如果这不是一个 HEAD 请求，而且服务器希望能够讲清楚为何请求不能被执行，那么就应该在实体内描述拒绝的原因。当然服务器也可以返回一个404响应，假如它不希望让客户端获得任何信息。 404 请求失败，请求所希望得到的资源未被在服务器上发现。没有信息能够告诉用户这个状况到底是暂时的还是永久的。假如服务器知道情况的话，应当使用410状态码来告知旧资源因为某些内部的配置机制问题，已经永久的不可用，而且没有任何可以跳转的地址。404这个状态码被广泛应用于当服务器不想揭示到底为何请求被拒绝或者没有其他适合的响应可用的情况下。 405 请求行中指定的请求方法不能被用于请求相应的资源。该响应必须返回一个Allow 头信息用以表示出当前资源能够接受的请求方法的列表。 鉴于 PUT，DELETE 方法会对服务器上的资源进行写操作，因而绝大部分的网页服务器都不支持或者在默认配置下不允许上述请求方法，对于此类请求均会返回405错误。 406 请求的资源的内容特性无法满足请求头中的条件，因而无法生成响应实体。 除非这是一个 HEAD 请求，否则该响应就应当返回一个包含可以让用户或者浏览器从中选择最合适的实体特性以及地址列表的实体。实体的格式由 Content-Type 头中定义的媒体类型决定。浏览器可以根据格式及自身能力自行作出最佳选择。但是，规范中并没有定义任何作出此类自动选择的标准。 407 与401响应类似，只不过客户端必须在代理服务器上进行身份验证。代理服务器必须返回一个 Proxy-Authenticate 用以进行身份询问。客户端可以返回一个 Proxy-Authorization 信息头用以验证。参见RFC 2617。 408 请求超时。客户端没有在服务器预备等待的时间内完成一个请求的发送。客户端可以随时再次提交这一请求而无需进行任何更改。 409 由于和被请求的资源的当前状态之间存在冲突，请求无法完成。这个代码只允许用在这样的情况下才能被使用：用户被认为能够解决冲突，并且会重新提交新的请求。该响应应当包含足够的信息以便用户发现冲突的源头。 冲突通常发生于对 PUT 请求的处理中。例如，在采用版本检查的环境下，某次 PUT 提交的对特定资源的修改请求所附带的版本信息与之前的某个（第三方）请求向冲突，那么此时服务器就应该返回一个409错误，告知用户请求无法完成。此时，响应实体中很可能会包含两个冲突版本之间的差异比较，以便用户重新提交归并以后的新版本。 410 被请求的资源在服务器上已经不再可用，而且没有任何已知的转发地址。这样的状况应当被认为是永久性的。如果可能，拥有链接编辑功能的客户端应当在获得用户许可后删除所有指向这个地址的引用。如果服务器不知道或者无法确定这个状况是否是永久的，那么就应该使用404状态码。除非额外说明，否则这个响应是可缓存的。 410响应的目的主要是帮助网站管理员维护网站，通知用户该资源已经不再可用，并且服务器拥有者希望所有指向这个资源的远端连接也被删除。这类事件在限时、增值服务中很普遍。同样，410响应也被用于通知客户端在当前服务器站点上，原本属于某个个人的资源已经不再可用。当然，是否需要把所有永久不可用的资源标记为’410 Gone’，以及是否需要保持此标记多长时间，完全取决于服务器拥有者。 411 服务器拒绝在没有定义 Content-Length 头的情况下接受请求。在添加了表明请求消息体长度的有效 Content-Length 头之后，客户端可以再次提交该请求。 412 服务器在验证在请求的头字段中给出先决条件时，没能满足其中的一个或多个。这个状态码允许客户端在获取资源时在请求的元信息（请求头字段数据）中设置先决条件，以此避免该请求方法被应用到其希望的内容以外的资源上。 413 服务器拒绝处理当前请求，因为该请求提交的实体数据大小超过了服务器愿意或者能够处理的范围。此种情况下，服务器可以关闭连接以免客户端继续发送此请求。 如果这个状况是临时的，服务器应当返回一个 Retry-After 的响应头，以告知客户端可以在多少时间以后重新尝试。 414 请求的URI 长度超过了服务器能够解释的长度，因此服务器拒绝对该请求提供服务。这比较少见，通常的情况包括： 本应使用POST方法的表单提交变成了GET方法，导致查询字符串（Query String）过长。 重定向URI “黑洞”，例如每次重定向把旧的 URI 作为新的 URI 的一部分，导致在若干次重定向后 URI 超长。 客户端正在尝试利用某些服务器中存在的安全漏洞攻击服务器。这类服务器使用固定长度的缓冲读取或操作请求的 URI，当 GET 后的参数超过某个数值后，可能会产生缓冲区溢出，导致任意代码被执行[1]。没有此类漏洞的服务器，应当返回414状态码。 415 对于当前请求的方法和所请求的资源，请求中提交的实体并不是服务器中所支持的格式，因此请求被拒绝。 416 如果请求中包含了 Range 请求头，并且 Range 中指定的任何数据范围都与当前资源的可用范围不重合，同时请求中又没有定义 If-Range 请求头，那么服务器就应当返回416状态码。 假如 Range 使用的是字节范围，那么这种情况就是指请求指定的所有数据范围的首字节位置都超过了当前资源的长度。服务器也应当在返回416状态码的同时，包含一个 Content-Range 实体头，用以指明当前资源的长度。这个响应也被禁止使用 multipart/byteranges 作为其 Content-Type。 417 在请求头 Expect 中指定的预期内容无法被服务器满足，或者这个服务器是一个代理服务器，它有明显的证据证明在当前路由的下一个节点上，Expect 的内容无法被满足。 421 从当前客户端所在的IP地址到服务器的连接数超过了服务器许可的最大范围。通常，这里的IP地址指的是从服务器上看到的客户端地址（比如用户的网关或者代理服务器地址）。在这种情况下，连接数的计算可能涉及到不止一个终端用户。 422 从当前客户端所在的IP地址到服务器的连接数超过了服务器许可的最大范围。通常，这里的IP地址指的是从服务器上看到的客户端地址（比如用户的网关或者代理服务器地址）。在这种情况下，连接数的计算可能涉及到不止一个终端用户。 422 请求格式正确，但是由于含有语义错误，无法响应。（RFC 4918 WebDAV）423 Locked 当前资源被锁定。（RFC 4918 WebDAV） 424 由于之前的某个请求发生的错误，导致当前请求失败，例如 PROPPATCH。（RFC 4918 WebDAV） 425 在WebDav Advanced Collections 草案中定义，但是未出现在《WebDAV 顺序集协议》（RFC 3658）中。 426 客户端应当切换到TLS/1.0。（RFC 2817） 449 由微软扩展，代表请求应当在执行完适当的操作后进行重试。 500 服务器遇到了一个未曾预料的状况，导致了它无法完成对请求的处理。一般来说，这个问题都会在服务器的程序码出错时出现。 501 服务器不支持当前请求所需要的某个功能。当服务器无法识别请求的方法，并且无法支持其对任何资源的请求。 502 作为网关或者代理工作的服务器尝试执行请求时，从上游服务器接收到无效的响应。 503 由于临时的服务器维护或者过载，服务器当前无法处理请求。这个状况是临时的，并且将在一段时间以后恢复。如果能够预计延迟时间，那么响应中可以包含一个 Retry-After 头用以标明这个延迟时间。如果没有给出这个 Retry-After 信息，那么客户端应当以处理500响应的方式处理它。 注意：503状态码的存在并不意味着服务器在过载的时候必须使用它。某些服务器只不过是希望拒绝客户端的连接。 504 作为网关或者代理工作的服务器尝试执行请求时，未能及时从上游服务器（URI标识出的服务器，例如HTTP、FTP、LDAP）或者辅助服务器（例如DNS）收到响应。 注意：某些代理服务器在DNS查询超时时会返回400或者500错误 505 服务器不支持，或者拒绝支持在请求中使用的 HTTP 版本。这暗示着服务器不能或不愿使用与客户端相同的版本。响应中应当包含一个描述了为何版本不被支持以及服务器支持哪些协议的实体。 506 由《透明内容协商协议》（RFC 2295）扩展，代表服务器存在内部配置错误：被请求的协商变元资源被配置为在透明内容协商中使用自己，因此在一个协商处理中不是一个合适的重点。 507 服务器无法存储完成请求所必须的内容。这个状况被认为是临时的。WebDAV (RFC 4918) 509 服务器达到带宽限制。这不是一个官方的状态码，但是仍被广泛使用。 510 获取资源所需要的策略并没有没满足。（RFC 2774）]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[定时任务调度设计与实现]]></title>
    <url>%2F2018%2F02%2F17%2F%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1%E8%B0%83%E5%BA%A6%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[很多时候业务系统中有定时任务或者定时超时处理的需求，当任务量很大时，需要维护大量的timer，或者进行低效的扫描，本文针对此类业务场景与大家分享一种高效实时的解决方式。 缘起很多时候，业务中有“在一段时间之后，完成一个工作任务”的需求。 例如：客户下单待支付，如果超过30分钟未支付，系统自动关闭该订单。 入职以来碰到多次CPU100%，数据库查询超时异常等情况，疑似定时任务扫库引起。 常见方案：​ 1，“轮询扫描法”：启动一个cron定时任务，定期跑一次，将未支付的到期订单关闭，如果数据量很大，需要分页查询，分页update，这将会是一个for循环。 ​ 2，“多timer触发法”：针对每个任务启动一个timer，30分钟后触发是否需要关闭。 方案1不足：​ 1，轮询效率比较低 ​ 2，每次扫库，已经被执行过的记录，仍然会被扫描（只是不会出现在结果集中），有重复计算的嫌疑 ​ 3，时效性不够好，如果每小时轮询一次，最差的情况下，时间误差会达到1小时。 ​ 4，如果通过增加cron轮询频率来减少上面的时间误差，那轮询效率和重复计算的问题会进一步凸显。 方案2不足：​ 每个任务启动一个timer，比较耗资源，特别是在线量很大时，很容易CPU100%。 解决方案环形队列+时间轮 ​ 先对本图做个简单的解释，图中包含一个环形队列和多个任务集合，环形队列中每个slot对应一个时间单位，可以是小时、分钟、秒等，每个slot下挂着一个Set，用于存储待处理任务。当timer启动后，假设每隔1秒在环形队列中移动一格，当currentIndex指向一个slot时，可以检测该slot下有无待处理任务。在Set中的Task具备两个重要属性：cycleNum（当currentIndex第几圈扫描到这个slot时执行任务），taskFunction（需要执行的任务指针）。 ​ 所以当需要触发一个延时任务时，我们只需做到两点：1，计算这个Task应该放在哪一个slot；2，计算这个Task的cycleNum，假设环形队列设置为3600格，currentIndex每秒移动一格，那任务希望3610秒后执行，则3610/3600=1，cycleNum=1，该task放入第10个slot中。 方案优点​ 1，无需再轮询全部业务订单，效率高。 ​ 2，一个订单，任务只执行一次。 ​ 3，时效性好，精确到秒（控制timer移动频率可以控制精度）。 方案解析 ​ 上图的方案优点在于将任务的存储和时间进行解耦，而以什么方式存储任务数据则是该方案的关键。在RocketMQ（商业版）中的实现是将任务数据以链表的方式写入磁盘，每个任务中都有一个指向下一个任务的磁盘offset，只要拿到链表的头就可以获取整个任务链表，但该方案的缺点是部署时需要依赖硬盘，在当前容器化以及容量动态扩容的趋势下，一个普通应用依赖一块固定磁盘，对运维和部署带来额外复杂度。 优化设计​ 基于上述问题，存储方式可以改为以Hbase，Redis来存储结构化数据，将上述磁盘offset换成任务ID，时间轮上关联链表头的ID，从而解耦对磁盘的依赖。于是方案变成下图： ​ 该方案至此在数据量不是很大的应用中已经够用，但在大量数据下时依然存在以下问题： ​ 1，单一链表无法并行提取，从而影响提取效率，对于某个时刻有大量定时任务的时候，定时任务处理的延迟会比较严重。 ​ 2，在调度集群服务中各个节点拥有自己的时间轮，那么在集群里面每个节点重启之后如何恢复？集群扩容&amp;缩容如何自动管理？ 任务链表分区–加速单一链表提取​ 链表的好处是在内存中不用存储整个任务列表，只需要存储简单的ID，这样减少了内存的消耗，但单一链表提取效率有限，这里利用分区原理，将某个时刻的单一链表通过分区的方式拆分成多个链表，当将某个时间点的任务提取的时候，可以根据链表集合大小来并行处理，从而加速整个任务提取的速度。所以方案演变成下图 集群管理—集群节点的自我识别​ 在集群部署后节点重启后如何进行恢复？比如需要知道重启之前的时间轮刻度，需要知道重启之间时间轮刻度上的定时任务链表数据等，这里统一称之为时间轮元数据。由于各节点都是无状态的，所以各节点启动的时候，并不知道如何从存储服务中读取属于自己的元数据，这里就需要给集群中每个节点分配一个逻辑ID（因为动态扩容缩容问题，无法给每个节点分配固定IP或mac地址，奢侈），每个节点可以根据逻辑ID去存储服务获取对应的元数据。 ​ 自此引出一个问题，逻辑ID如何分配？那么集群中节点就要选举一个Master节点，由Master来统一分配逻辑ID，集群启动注册和分配ID过程如下图 ​ 在解决了集群节点获取元数据问题之后，可能会出现另一个问题，那就是某个时刻某个节点的到期任务量较大，从而导致集群中该节点的压力很大，为了能够合理利用集群计算能力，该方案可以进一步优化：将提取的到期任务链表集合通过软负载的方式分发给集群其他节点，同时由于任务的数据是集中存储的，只要其他节点能够拿到任务链表头的ID，便可以提取到该链表的所有任务，从而集群的压力也将被平摊。以下是该方案交互过程： 总结​ 在对该方案的解析过程中，我们发现该调度系统需要解决以下几个难点： ​ 1，时间轮算法实现 ​ 2，任务链表和任务数据存储、任务链表分区、提取、删除。 ​ 3，调度节点注册发现、Master选举策略 ​ 4，调度节点的逻辑ID分配，元数据恢复 ​ 5，到期任务链表集群软负载 外部链接：http://www.10tiao.com/html/249/201703/2651959961/1.html 外部链接：https://yq.aliyun.com/articles/581147 外部链接：https://zacard.net/2016/12/02/netty-hashedwheeltimer/]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫抓取代理IP]]></title>
    <url>%2F2018%2F01%2F17%2F%E7%88%AC%E8%99%AB%E6%8A%93%E5%8F%96%E4%BB%A3%E7%90%86IP%2F</url>
    <content type="text"><![CDATA[由于某些网站对会对爬虫做限制，因此常常需要通过代理将爬虫的实际IP隐蔽起来，代理也有分类，如透明代理，高匿代理等。本文主要讲述如何获取代理IP，并且如何存储和使用。 某些网站会免费提供代理IP，如下面的几个获取这些页面上的代理IP及端口也是通过爬虫抓取，下面以第一个网站http://www.xicidaili.com为例，解释如何获取并存储这些代理IP。一般的流程为：解析当前页面--&gt;存储当前页面的代理IP--&gt;跳转到下一页面，重复该流程即可。 ##解析页面首先要解析页面，由于网页中显示代理IP时是在表格中显示的，因此只需要通过找出网页源码中相关的表格元素即可。下面是通过python中的requests和bs4获取页面http://www.xicidaili.com/nt/上显示的IP及端口。 12345678910111213141516 import requestsfrom bs4 import BeautifulSoupuser_agent = &apos;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.106 Safari/537.36&apos;referer = &apos;http://www.xicidaili.com/&apos;headers = &#123;&apos;user-agent&apos;: user_agent, &apos;referer&apos;: referer&#125;target = &apos;http://www.xicidaili.com/nt/&apos;//获取页面源码r = requests.get(target, headers = headers)//解析页面源码soup = BeautifulSoup(r.text, &apos;lxml&apos;)for tr in soup.find_all(&apos;tr&apos;)[1:]:tds = tr.find_all(&apos;td&apos;)proxy = tds[1].text+&apos;:&apos;+tds[2].textprint proxy 输出如下： 1234567 36.235.1.189:3128219.141.225.149:80125.44.132.44:9999123.249.8.100:3128183.54.30.186:9999110.211.45.228:9000........... ###代理IP的存储上面代码获取的代理IP可以通过在代码一开始建立一个集合（set）来存储，这种情况适用于一次性使用这些代理IP，当程序发生异常或正常退出后，这些存储在内存中的代理IP也会丢失。但是爬虫中使用代理IP的情况又是非常多的，所以有必要把这些IP存储起来，从而可以让程序多次利用。这里主要通过redis数据库存储这些代理IP，redis是一个NOSQL数据库，具体使用参照官方文档，这里不做详细解释。下面是ConnectRedis.py文件，用于连接redis 12345678910import redisHOST = &apos;XXX.XXX.XXX.XXX&apos; # redis所在主机IPPORT = 6379 # redis服务监听的端口PASSWORD = &apos;XXXXXX&apos; # 连接redis的密码DB = 0 # IP存储的DB编号def get_connection():r = redis.Redis(host = HOST, port = PORT, password = PASSWORD, db= DB)return r 下面是在上面的代码基础上将IP存储到redis中， 123456789101112131415161718192021222324import requestsfrom bs4 import BeautifulSoupfrom ConnectRedis import get_connection//获取redis连接try:conn = get_connection()except Exception:print &apos;Error while connecting to redis&apos;returnuser_agent = &apos;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.106 Safari/537.36&apos;referer = &apos;http://www.xicidaili.com/&apos;headers = &#123;&apos;user-agent&apos;: user_agent, &apos;referer&apos;: referer&#125;target = &apos;http://www.xicidaili.com/nt/&apos;//获取页面源码r = requests.get(target, headers = headers)//解析页面源码soup = BeautifulSoup(r.text, &apos;lxml&apos;)for tr in soup.find_all(&apos;tr&apos;)[1:]:tds = tr.find_all(&apos;td&apos;)proxy = tds[1].text+&apos;:&apos;+tds[2].textconn.sadd(&quot;ip_set&quot;, proxy)print &apos;%s added to ip set&apos;%proxy 上面的conn.sadd(“ip_set”, proxy)将代理proxy加入到redis的集合”ip_set”，这个集合需要预先在redis中创建，否则会出错。 ###页面跳转上面的代码获取的只是一个页面上显示的代理，显然这个数量不够，一般通过当前页面中的下一页的超链接可以跳转到下一页，但是我们测试的由于每页的的url都有规律，都是http://www.xicidaili.com/nt/page_number,其中的page_number表示当前在哪一页，省略时为第一页。因此，通过一个for循环嵌套上面的代码即可获取多个页面的代理。但是更一般的方法是通过在当前页面获取下一页的超链接而跳转到下一页。 ###代理IP的使用当我们需要通过代理访问某一网站时，首先需要从redis中随机选出一个代理ip，然后尝试通过代理ip是否能连到我们需要访问的目标网站，因为这些代理IP是公共使用的，所以往往也会被封的很快，假如通过代理无法访问目标网站，那么就要从数据库中删除这个代理IP。反之即可通过此代理访问目标网站下面是实现上面所说流程的代码： 1234567891011121314151617181920212223242526272829import requestsfrom ConnectRedis import get_connection//判断IP是否能访问目标网站def is_valid(url, ip):proxy = &#123;&apos;http&apos;: &apos;http://%s&apos; %ip&#125;user_agent = &apos;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.106 Safari/537.36&apos;headers = &#123;&apos;user-agent&apos;: user_agent&#125;try:r = requests.get(url, headers = headers, proxies = proxy, timeout = 6)return Trueexcept Exception:return Falseif __name__ == &apos;__main__&apos;:my_proxy, proxies, ip_set = None, None, &apos;amazon_ips&apos;conn = get_connection()target = &apos;https://www.amazon.com/&apos;while not is_valid(target, my_proxy):if my_proxy:conn.srem(ip_set, my_proxy) #删除无效的代理IPif proxies:my_proxy = proxies.pop()else:proxies = conn.srandmember(ip_set, 5) #从redis中随机抽5个代理ipmy_proxy = proxies.pop()print &apos;valid proxy %s&apos; %my_proxy requests.get(url, headers = headers, proxies = proxy, timeout = 6)是通过代理去访问目标网站，超时时间设为6s，也就是说在6秒内网站没有回应或返回错误信息就认为这个代理无效。除此之外，在爬取免费提供代理的网站上的代理IP的时候，爬取的速度不要太快，其中的一个原因是爬取太快有可能会被封，另外一个原因是如果每个人都无间隙地从这种网站上爬取，那么网站的负担会比较大，甚至有可能垮掉，因此采用一个可持续爬取的策略非常有必要，我爬取的时候是没爬完一个页面后让程序sleep大概2分钟，这样下来不会被封而且爬取的代理的量也足够使用。实际中可以根据自己使用代理的频率来进行调整。]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker常用指令]]></title>
    <url>%2F2017%2F12%2F12%2FDocker%E5%B8%B8%E7%94%A8%E6%8C%87%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[Docker常用指令 容器清理 12345docker ps -a | grep "Exited" | awk '&#123;print $1 &#125;'|xargs docker stopdocker ps -a | grep "Exited" | awk '&#123;print $1 &#125;'|xargs docker rmdocker images|grep none|awk '&#123;print $3 &#125;'|xargs docker rmi 查看虚悬镜像 1docker images -f dangling=true 删除虚悬镜像 1docker rmi $(docker images -q -f dangling=true) 查看在mongo:3.2之后建立的镜像 1docker images -f since=mongo:3.2 查看在mongo:3.2之前建立的镜像 1docker images -f before=mongo:3.2 在容器基础上构建新镜像 12345678docker commit [选项] &lt;容器ID或容器名&gt; [&lt;仓库名&gt;[:&lt;标签&gt;]]docker commit \--author "Tao Wang &lt;twang2218@gmail.com&gt;" \--message "修改了默认网页" \webserver \nginx:v2sha256:07e33465974800ce65751acc279adc6ed2dc5ed4e0838f8b86f0c87aa1795214 运行容器 1docker run --name web2 -d -p 81:80 nginx:v2 删除镜像 1docker rmi [选项] &lt;镜像1&gt; [&lt;镜像2&gt; ...] 删除容器 1docker rm 清除所有终止状态的容器 1docker rm $(docker ps -a -q) 创建数据卷容器 12sudo docker run -d -v /dbdata --name dbdata training/postgresecho Data-only container for postgres 挂载数据卷容器中的数据卷 1234$ sudo docker run -d --volumes-from dbdata --name db1 training/postgres$ sudo docker run -d --volumes-from dbdata --name db2 training/postgres build: 构建镜像 1docker build -t oauth2-server:0.6.0 . run: 运行镜像 1234//守护进程形式 docker run -d -p 8180:8180 --name oa oauth2-server:0.6.0//shell交互形式(本服务不支持，可运行其他镜像) docker run -i -t --rm java:8 /bin/bash 查看镜像 1docker images 查看容器 1docker ps -a 查看日志 12345docker logs -f --tail=100 ContainerId * -f, --follow=false 跟踪日志输出。 * -t, --timestamps=false 显示时间戳。 * --tail="all" 输出日志尾部特定行(默认是所有)。 * --since="" 时间段 查看磁盘挂载 1docker inspect -f &#123;&#123;.Volumes&#125;&#125; ContainerId 导出日志目录 1docker cp ContainerId:/var/log/oauth2-server /local/path 谨慎操作 删除一个镜像 1docker rmi -f ImageId 删除所有镜像 1docker rmi -f $(docker images -q) 删除一个容器 1docker rm -v -f ContainerId 删除所有容器 1docker rm -v -f $(docker ps -a -q) 删除旧的容器 1docker ps -a | grep 'weeks ago' | awk '&#123;print $1&#125;' | xargs docker rm -v -f 删除停止状态的容器 1docker rm -v -f $(docker ps -a -q -f status=exited) 设置容器的磁盘空间（需要重启docker） 1 docker -d --storage-opt dm.basesize=50G 在容器中执行命令 123456 docker exec -i -t ContainerName myshell * -d, --detach=false Detached mode: run command in the background* -i, --interactive=false Keep STDIN open even if not attached* -t, --tty=false Allocate a pseudo-TTY* -u, --user= Username or UID (format: &lt;name|uid&gt;[:&lt;group|gid&gt;])如: docker exec -i -t ContainerName /bin/bash 提交容器当前状态到镜像 123456 docker commit [OPTIONS] CONTAINER [REPOSITORY[:TAG]]* -a, --author="" Author (e.g., "John Hannibal Smith &lt;hannibal@a-team.com&gt;")* -c, --change=[] Apply specified Dockerfile instructions while committing the image* -m, --message="" Commit message* -p, --pause=true Pause container during commit 保存/备份镜像到本地 12* docker save -o /local/path/fedora-latest.tar fedora:latest* docker save fedora:latest &gt; /local/path/fedora-latest.tar 加载本地镜像文件到docker 12* docker load --input fedora.tar* docker load &lt; fedora.tar 容器与主机文件/目录传输 12* docker cp [options] CONTAINER:PATH LOCALPATH* docker cp [options] LOCALPATH CONTAINER:PATH 连接到运行的容器中（谨慎使用，不推荐） 123 * docker attach [OPTIONS] CONTAINERID* --no-stdin=false Do not attach STDIN* --sig-proxy=true Proxy all received signals to the process 退出attach 默认情况下，如果使用ctrl-c退出container,那么container也会stop 按ctrl-p ctrl-q可以退出到宿主机，而保持container仍然在运行 查看容器或镜像的详细信息 1234 docker inspect [OPTIONS] CONTAINER|IMAGE [CONTAINER|IMAGE...]* -f, --format="" Format the output using the given go template* --type=container|image Return JSON for specified type, permissible values are "image" or "container" docker run 可选参数 -c/–cpu-shares 512 每个CPU的配额（1024允许占用全部CPU） –cpuset=0,1 只允许运行在前两个CPU上（–cpu 2） –cpu 4 使用四核（加载run命令的末尾） -m 128m 内存分配 详细参考Docker容器资源管理 –dns 覆盖容器默认dns配置 –mac-address 覆盖容器默认mac地址配置 –add-host 添加hosts（如–add-host db-static:86.75.30.9） –security-opt 指定安全策略（主机配置） –privileged Docker将拥有访问主机所有设备的权限 –device 指定容器可访问的设备（如–device=/dev/snd:/dev/snd:rwx rwx为权限） –rm 容器停止后删除容器(不与-d一起使用) -e 设置环境变量（如-e “deep=purple”） -h 来设定hostname –link name:alias 连接其他容器 -v $HOSTDIR:$DOCKERDIR 挂载主机目录到容器目录 –read-only 设置容器只读 docker run –net 参数 none。关闭容器内的网络连接 将网络模式设置为none时，这个容器将不允许访问任何外部router。 这个容器内部只会有一个loopback接口，而且不存在任何可以访问外部网络的router。 bridge。通过veth接口来连接容器，默认配置。 Docker默认会将容器设置为bridge模式。此时在主机上面将会存在一个docker0的网络接口， 同时会针对容器创建一对veth接口。其中一个veth接口是在主机充当网卡桥接作用，另外一个veth接口存在于容器的命名空间中， 并且指向容器的loopback。Docker会自动给这个容器分配一个IP，并且将容器内的数据通过桥接转发到外部。 host。允许容器使用host的网络堆栈信息。 注意：这种方式将允许容器访问host中类似D-BUS之类的系统服务，所以认为是不安全的。 当网络模式设置为host时，这个容器将完全共享host的网络堆栈。host所有的网络接口将完全对容器开放。 容器的主机名也会存在于主机的hostname中。这时，容器所有对外暴露的端口和对其它容器的连接，将完全失效。 container。使用另外一个容器的网络堆栈信息。 当网络模式设置为Container时，这个容器将完全复用另外一个容器的网络堆栈。 同时使用时这个容器的名称必须要符合下面的格式：–net container:&lt;name|id&gt;. 查看镜像历史 1234 docker history [OPTIONS] IMAGE-H, --human=true Print sizes and dates in human readable format--no-trunc=false Don't truncate output-q, --quiet=false Only show numeric IDs 导出镜像 123 docker export [OPTIONS] CONTAINERdocker export exampleimage &gt; exampleimage.tardocker export --output="exampleimage.tar" exampleimage 导入镜像 123 docker import URL|- [REPOSITORY[:TAG]]docker import http://example.com/exampleimage.tgzcat exampleimage.tgz | docker import - exampleimagelocal:new 其他 123456789101112131415docker tag IMAGE[:TAG] [REGISTRYHOST/][USERNAME/]NAME[:TAG]docker stop container --stops it.docker start container --will start it again.docker restart container --restarts a container.docker kill container --sends a SIGKILL to a container.docker wait container --blocks until container stops.docker port container --shows public facing port of container.docker top container --shows running processes in container.docker stats container --shows containers' resource usage statistics.docker diff container --shows changed files in the container's FS.docker events --gets events from container.docker login --to login to a registry.docker search --searches registry for image.docker pull pulls an image from registry to local machine.docker push --pushes an image to the registry from local machine. 本地生成镜像，打包发布本地项目根目录下创建Dockerfile 123456FROM java:openjdk-8-jreWORKDIR /usr/src/COPY target/academic-0.0.1.jar /usr/src/CMD ["java", "-Duser.timezone=GMT+08", "-jar", "academic-0.0.1.jar"]RUN ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtimeEXPOSE 8072 在Dockerfile所在目录执行创建镜像指令 1docker build -t academic . (-t academic是为镜像指定名称为academic) 打包镜像为tar文件 1docker save academic &gt; academic.tar 上传academic.tar至服务器ssh到服务器，执行导出镜像命令 1docker load &lt; academic.tar 编写docker-compose.yml文件 1执行dc up -d 即可]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker加速器配置]]></title>
    <url>%2F2017%2F12%2F07%2FDocker%E5%8A%A0%E9%80%9F%E5%99%A8%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[用了Docker的都知道在使用docker pull来拉取镜像时的速度都非常慢，而且还会经常报错（如：超时、镜像不存在等错误）。主要原因大家都知道：Registry在国内，网络影响太严重。还好国内的一些大企业对此做了些解决办法：加速器，比较常用的有阿里云和DaoCloud。要使用加速器都需要先注册一个对应的账号，我这里已经在这两个网站上都注册过了。 Centos7中配置Docker加速器 DaoCloud加速器1curl -sSL https://get.daocloud.io/daotools/set_mirror.sh | sh -s http://XXX.m.daocloud.io 注意：XXX是自己的地址，注册账号后可以得到这个地址，可以参考官方文档：《配置 Docker 加速器》，上面这条命令适用于Ubuntu14.04、Debian、CentOS6 、CentOS7，并且可以帮助您配置registry-mirror加速地址并重启Docker Daemon。 阿里云加速器 1234 sudo cp -n /lib/systemd/system/docker.service /etc/systemd/system/docker.servicesudo sed -i &quot;s|ExecStart=/usr/bin/docker daemon|ExecStart=/usr/bin/docker daemon --registry-mirror=https://XXX.mirror.aliyuncs.com|g&quot; /etc/systemd/system/docker.servicesudo systemctl daemon-reloadsudo service docker restart 注意：XXX也是注册阿里云账号后的一个地址，可以参考官方文档：《配置Docker加速器》 需要特别注意的是：阿里云在Linux系统中的加速器只支持Docker 1.9版本以上，Centos7以上，Ubuntu 12以上，个人感觉阿里云在Linux系统中的支持不如DaoCloud的完美。 Windows中配置Docker加速器 DaoCloud加速器为已存在的docker-machine配置加速器（假设machine名称为tmp-machine） 123docker-machine ssh tmp-machinecd /var/lib/boot2dockervi profile 在EXTRA_ARGS中配置加速地址，即修改为： 123EXTRA_ARGS='--label provider=virtualbox--registry-mirror http://XXX.m.daocloud.io' 注意：XXX需要更换成自己的加速地址（–registry-mirror后面是空格不是等号），保存并退出，然后输入命令：docker-machine restart tmp-machine重启虚拟主机。 docker-machine create时配置加速器 1docker-machine create -d virtualbox --engine-registry-mirror=http://XXX.m.daocloud.io tmp-machine 注意：XXX需要更换成自己的加速地址，这样创建了一个名为tmp-machine的docker虚拟机，并设置了相应的加速器。 阿里云加速器 Windows中使用阿里云的加速器配置方法与配置DaoCloud加速器的方法是一样的，需要注意的是阿里云的加速器地址是以https://开头，格式如：https://XXX.mirror.aliyuncs.com，将上面所说的加速器地址更新成阿里云的就可以了。个人用了加速器后的体会是：在Linux系统中DaoCloud的加速器比阿里云的要好用，在Windows系统中阿里云的加速器比DaoCloud的速度快。]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[离线安装docker最新版]]></title>
    <url>%2F2017%2F11%2F25%2F%E7%A6%BB%E7%BA%BF%E5%AE%89%E8%A3%85docker%E6%9C%80%E6%96%B0%E7%89%88%2F</url>
    <content type="text"><![CDATA[离线安装Docker最新版 如果安装了以前版本，还要删除以下这个包。 1container-selinux.noarch 2:2.10-2.el7 离线安装三个rpm 123-rw-r--r--. 1 root root 19524568 May 15 10:04 docker-ce-17.03.1.ce-1.el7.centos.x86_64.rpm-rw-r--r--. 1 root root 29108 May 15 10:04 docker-ce-selinux-17.03.1.ce-1.el7.centos.noarch.rpm-rw-r--r--. 1 root root 50076 May 16 08:32 libtool-ltdl-2.4.2-22.el7_3.x86_64.rpm 下载地址： 123https://download.docker.com/linux/https://centos.pkgs.org/7/centos-updates-x86_64/libtool-ltdl-2.4.2-22.el7_3.x86_64.rpm.htmlhttp://mirror.centos.org/centos/7/updates/x86_64/Packages/libtool-ltdl-2.4.2-22.el7_3.x86_64.rpm]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS7安装docker-compose]]></title>
    <url>%2F2017%2F11%2F21%2FCentOS7%E5%AE%89%E8%A3%85docker-compose%2F</url>
    <content type="text"><![CDATA[compose是用来在docker中定义和运行复杂应用的小工具,比如在一个文件中定义多个容器,只用一行命令就可以让一切就绪并运行. ###安装pip:这里显示，找不到相应的包？？说没有python-pip软件包可以安装。这是因为像centos这类衍生出来的发行版，他们的源有时候内容更新的比较滞后，或者说有时候一些扩展的源根本就没有。所以在使用yum来search python-pip的时候，会说没有找到该软件包。因此为了能够安装这些包，需要先安装扩展源EPEL。EPEL(http://fedoraproject.org/wiki/EPEL) 是由 Fedora 社区打造，为 RHEL 及衍生发行版如 CentOS、Scientific Linux 等提供高质量软件包的项目。首先安装epel扩展源： 1sudo yum -y install epel-release 然后安装python-pip 1sudo yum install python2-pip.noarch ###对安装好的pip进行一次升级 1sudo pip install --upgrade pip 安装docker-compose 1pip install docker-compose 运行docker-compose 出现报错pkg_resources.DistributionNotFound: backports.ssl-match-hostname&gt;=3.5使用pip 更新backports.ssl-match-hostname的版本 1pip install --upgrade backports.ssl_match_hostname 更新backports.ssl_match_hostname 到3.5版本后问题解决]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入Java虚拟机学习之Java内存区域]]></title>
    <url>%2F2017%2F11%2F14%2F%E6%B7%B1%E5%85%A5Java%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%AD%A6%E4%B9%A0%E4%B9%8BJava%E5%86%85%E5%AD%98%E5%8C%BA%E5%9F%9F%2F</url>
    <content type="text"><![CDATA[介绍Java虚拟机内存区域:方法区，堆，虚拟机栈，本地方法栈，程序计数器的概念以及引起OOM(OutOfMemeoryError)的潜在原因。 方法区 方法区(Method Area)是各个线程共享的内存区域，它用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。虽然Java虚拟机规范把方法区描述为堆的一个逻辑部分，但是它却有一个别名叫做Non-Heap(非堆)，目的应该是与Java堆区分开来。 Java虚拟机规范对方法区的限制非常宽松，除了和Java堆一样不需要连续的内存和可以选择固定大小或者可扩展外，还可以选择不实现垃圾收集。相对而言，垃圾收集行为在这个区域是比较少出现的，但并非数据进入了方法区就和永久代的名字一样“永久”存在了。这区域的内存回收目标主要是针对常量池的回收和对类型的卸载，一般来说，这个区域的回收“成绩”比较难以令人满意，尤其是类型的卸载，条件相当苛刻，但是这部分区域的回收确实是必要的。在Sun公司的BUG列表中，曾出现过的若干个严重的BUG就是由于低版本的HotSpot虚拟机对此区域未完全回收而导致的内存泄露。 根据Java虚拟机规范的规定，当方法区无法满足内存分配需求时，将抛出OOM异常。 堆 Java堆(Java Heap)是Java虚拟机所管理的内存中最大的一块。Java堆是被所有线程共享的一块内存区域，在虚拟机启动时创建。此内存区域的唯一目的就是存放对象实例，几乎所有的对象实例都在这里分配内存。这一点在Java虚拟机规范中的描述是：所有的对象实例以及数据都要在堆上分配。但是随着JIT编译器的发展与逃逸分析技术逐渐成熟，栈上分配、标量变换优化技术将会导致一些微妙的变化发生，所有的对象都分配在堆上也渐渐变得不难么绝对了。 Java堆是垃圾收集器管理的主要区域，因此很多时候也被称做“GC堆”（Garbage Collected Heap，幸好国内没翻译成“垃圾堆”）。从内存回收的角度来看，由于现在收集器基本都采用分代收集算法，所以Java堆中还可以细分为：新生代和老年代；再细致一点的有Eden空间、From Survivor空间、To Survivor空间等。从内存分配的角度来看，线程共享的Java堆中可能划分出多个线程私有的分配缓冲区（Thread Local Allocation Buffer,TLAB）。不过无论如何划分，都与存放内容无关，无论哪个区域，存储的都仍然是对象实例，进一步划分的目的是为了更好地回收内存，或者更快地分配内存。 “根据Java虚拟机规范的规定，Java堆可以处于物理上不连续的内存空间中，只要逻辑上是连续的即可，就像我们的磁盘空间一样。在实现时，既可以实现成固定大小的，也可以是可扩展的，不过当前主流的虚拟机都是按照可扩展来实现的（通过-Xmx和-Xms控制）。如果在堆中没有内存完成实例分配，并且堆也无法再扩展时，将会抛出OutOfMemoryError异常。” 程序计数器 程序计数器（Program Counter Register）是一块较小的内存空间，它可以看作是当前线程所执行的字节码的行号指示器。在虚拟机的概念模型里（仅是概念模型，各种虚拟机可能会通过一些更高效的方式去实现），字节码解释器工作时就是通过改变这个计数器的值来选取下一条需要执行的字节码指令，分支、循环、跳转、异常处理、线程恢复等基础功能都需要依赖这个计数器来完成。 由于Java虚拟机的多线程是通过线程轮流切换并分配处理器执行时间的方式来实现的，在任何一个确定的时刻，一个处理器（对于多核处理器来说是一个内核）都只会执行一条线程中的指令。因此，为了线程切换后能恢复到正确的执行位置，每条线程都需要有一个独立的程序计数器，各条线程之间计数器互不影响，独立存储，我们称这类内存区域为“线程私有”的内存。 如果线程正在执行的是一个Java方法，这个计数器记录的是正在执行的虚拟机字节码指令的地址；如果正在执行的是Native方法，这个计数器值则为空（Undefined）。此内存区域是唯一一个在Java虚拟机规范中没有规定任何OutOfMemoryError情况的区域。 虚拟机栈 与程序计数器一样，Java虚拟机栈（Java Virtual Machine Stacks）也是线程私有的，它的生命周期与线程相同。虚拟机栈描述的是Java方法执行的内存模型：每个方法在执行的同时都会创建一个栈帧（Stack Frame[1]）用于存储局部变量表、操作数栈、动态链接、方法出口等信息。每一个方法从调用直至执行完成的过程，就对应着一个栈帧在虚拟机栈中入栈到出栈的过程。 经常有人把Java内存区分为堆内存（Heap）和栈内存（Stack），这种分法比较粗糙，Java内存区域的划分“实际上远比这复杂。这种划分方式的流行只能说明大多数程序员最关注的、与对象内存分配关系最密切的内存区域是这两块。其中所指的“堆”笔者在后面会专门讲述，而所指的“栈”就是现在讲的虚拟机栈，或者说是虚拟机栈中局部变量表部分。局部变量表存放了编译期可知的各种基本数据类型（boolean、byte、char、short、int、float、long、double）、对象引用（reference类型，它不等同于对象本身，可能是一个指向对象起始地址的引用指针，也可能是指向一个代表对象的句柄或其他与此对象相关的位置）和returnAddress类型（指向了一条字节码指令的地址）。其中64位长度的long和double类型的数据会占用2个局部变量空间（Slot），其余的数据类型只占用1个。局部变量表所需的内存空间在编译期间完成分配，当进入一个方法时，这个方法需要在帧中分配多大的局部变量空间是完全确定的，在方法运行期间不会改变局部变量表的大小。 在Java虚拟机规范中，对这个区域规定了两种异常状况：如果线程请求的栈深度大于虚拟机所允许的深度，将抛出StackOverflowError异常；如果虚拟机栈可以动态扩展（当前大部分的Java虚拟机都可动态扩展，只不过Java虚拟机规范中也允许固定长度的虚拟机栈），如果扩展时无法申请到足够的内存，就会抛出OutOfMemoryError异常。 本地方法栈 本地方法栈（Native Method Stack）与虚拟机栈所发挥的作用是非常相似的，它们之间的区别不过是虚拟机栈为虚拟机执行Java方法（也就是字节码）服务，而本地方法栈则为虚拟机使用到的Native方法服务。在虚拟机规范中对本地方法栈中方法使用的语言、使用方式与数据结构并没有强制规定，因此具体的虚拟机可以自由实现它。甚至有的虚拟机（譬如Sun HotSpot虚拟机）直接就把本地方法栈和虚拟机栈合二为一。与虚拟机栈一样，本地方法栈区域也会抛出StackOverflowError和OutOfMemoryError异常。 运行时常量池 “运行时常量池（Runtime Constant Pool）是方法区的一部分。Class文件中除了有类的版本、字段、方法、接口等描述信息外，还有一项信息是常量池（Constant Pool Table），用于存放编译期生成的各种字面量和符号引用，这部分内容将在类加载后进入方法区的运行时常量池中存放。 Java虚拟机对Class文件每一部分（自然也包括常量池）的格式都有严格规定，每一个字节用于存储哪种数据都必须符合规范上的要求才会被虚拟机认可、装载和执行，但对于运行时常量池，Java虚拟机规范没有做任何细节的要求，不同的提供商实现的虚拟机可以按照自己的需要来实现这个内存区域。不过，一般来说，除了保存Class文件中描述的符号引用外，还会把翻译出来的直接引用也存储在运行时常量池中[1]。 运行时常量池相对于Class文件常量池的另外一个重要特征是具备动态性，Java语言并不要求常量一定只有编译期才能产生，也就是并非预置入Class文件中常量池的内容才能进入方法区运行时常量池，运行期间也可能将新的常量放入池中，这种特性被开发人员利用得比较多的便是String类的intern（）方法。既然运行时常量池是方法区的一部分，自然受到方法区内存的限制，当常量池无法再申请到内存时会抛出OOM异常 直接内存 直接内存（Direct Memory）并不是虚拟机运行时数据区的一部分，也不是Java虚拟机规范中定义的内存区域。但是这部分内存也被频繁地使用，而且也可能导致OutOfMemoryError异常出现，所以我们放到这里一起讲解。 在JDK 1.4中新加入了NIO（New Input/Output）类，引入了一种基于通道（Channel）与缓冲区（Buffer）的I/O方式，它可以使用Native函数库直接分配堆外内存，然后通过一个存储在Java堆中的DirectByteBuffer对象作为这块内存的引用进行操作。这“样能在一些场景中显著提高性能，因为避免了在Java堆和Native堆中来回复制数据。 显然，本机直接内存的分配不会受到Java堆大小的限制，但是，既然是内存，肯定还是会受到本机总内存（包括RAM以及SWAP区或者分页文件）大小以及处理器寻址空间的限制。服务器管理员在配置虚拟机参数时，会根据实际内存设置-Xmx等参数信息，但经常忽略直接内存，使得各个内存区域总和大于物理内存限制（包括物理的和操作系统级的限制），从而导致动态扩展时出现OutOfMemoryError异常。 原文作者： 周志明 深入理解Java虚拟机：JVM高级特性与最佳实践（第2版）]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[笛卡尔积算法]]></title>
    <url>%2F2017%2F11%2F13%2F%E7%AC%9B%E5%8D%A1%E5%B0%94%E7%A7%AF%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[笛卡尔积介绍。 ##STEP 1 : 什么是笛卡尔积来自维基百科-笛卡儿积的解释: 在数学中，两个集合X和Y的笛卡儿积（Cartesian product），又称直积，在集合论中表示为X × Y，是所有可能的有序对组成的集合，其中有序对的第一个对象是X的成员，第二个对象是Y的成员。 来自百度百科-笛卡儿积的解释: 笛卡尔乘积是指在数学中，两个集合X和Y的笛卡尓积（Cartesian product），又称直积，表示为X × Y，第一个对象是X的成员而第二个对象是Y的所有可能有序对的其中一个成员。 举个比较常见的例子: 如果集合X是13个元素的点数集合{ A, K, Q, J, 10, 9, 8, 7, 6, 5, 4, 3, 2 }，而集合Y是4个元素的花色集合{♠, ♥, ♦, ♣}，则这两个集合的笛卡儿积是有52个元素的标准扑克牌的集合{ (A, ♠), (K, ♠), …, (2, ♠), (A, ♥), …, (3, ♣), (2, ♣) }。 ##STEP 2 : java 实现笛卡尔积算法实际上网络上有很多的笛卡尔积算法的实现,在此我就取其中一种方法进行演示: 循环内，每次只有一列向下移一个单元格，就是CounterIndex指向的那列。 如果该列到尾部了，则这列index重置为0，而CounterIndex则指向前一列，相当于进位，把前列的index加一。 最后，由生成的行数来控制退出循环。123456789101112131415public class Test &#123;public static void main(String[] args) &#123;String[] x = &#123;"A", "K", "Q", "J", "10", "9", "8", "7", "6", "5", "4", "3", "2"&#125;;String[] y = &#123;"♠", "♥", "♦", "♣"&#125;; String[][] z = new String[y.length][x.length];for (int i = 0; i &lt; y.length; i++) &#123;z[i] = x;&#125;String[][] temp = cartesianProduct(z);for (int i = 0; i &lt; temp.length; i++) &#123;System.out.println(Arrays.toString(temp[i]));&#125;System.out.println(temp.length);&#125; private static String[][] cartesianProduct(String[][] args) {int total = 1;int counterIndex = args.length - 1;int[] counter = new int[args.length];for (int i = 0; i &lt; args.length; i++) {total *= args[i].length;counter[i] = 0;} String[][] result = new String[total][args.length];for (int i = 0; i &lt; total; i++) {for (int j = 0; j &lt; args.length; j++) {result[i][j] = args[j][counter[j]];}counterIndex = handle(counter, counterIndex, args);}return result;} private static int handle(int[] counter, int counterIndex, String[][] args) {counter[counterIndex]++;if (counter[counterIndex] &gt;= args[counterIndex].length) {counter[counterIndex] = 0;counterIndex–;if (counterIndex &gt;= 0) {handle(counter, counterIndex, args);}counterIndex = args.length - 1;}return counterIndex;}} 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475##STEP 3 : 笛卡尔积算法解决经典问题网络上有这样一个问题:&gt; 1 2 3 4 5 6 7 8 9这九个按顺序排列的数，要求在它们之间插入若干个 + , - 或者什么都不加使其结果正好等于100我们可以使用笛卡尔积算法将所有的组合计算出来,然后取其中结果为100的组合,具体实现如下: ```javapublic class Test &#123;static ScriptEngine jse = new ScriptEngineManager().getEngineByName("JavaScript");public static void main(String[] args) throws InterruptedException &#123;String[] x = &#123;"+", "-", ""&#125;;String[] y = &#123;"1", "2", "3", "4", "5", "6", "7", "8", "9"&#125;;String[][] z = new String[y.length][x.length];for (int i = 0; i &lt; y.length; i++) &#123;z[i] = x;&#125;//计算结果String[][] result = cartesianProduct(z);for (int i = 0; i &lt; result.length; i++) &#123;StringBuffer sb = new StringBuffer();for (int j = 0; j &lt; result[i].length; j++) &#123;sb.append(y[j]);sb.append(result[i][j]);&#125;sb.append(y[y.length - 1]);try &#123; //利用js脚本引擎直接转换字符串为计算表达式,从而获得计算结果Object obj = jse.eval(sb.toString());if (Integer.parseInt(String.valueOf(obj)) == 100)&#123;System.out.println(sb.toString() + " = 100");&#125;&#125; catch (Exception e) &#123;e.printStackTrace();&#125;&#125;&#125;private static String[][] cartesianProduct(String[][] args) &#123;int total = 1;int counterIndex = args.length - 1;int[] counter = new int[args.length];for (int i = 0; i &lt; args.length; i++) &#123;total *= args[i].length;counter[i] = 0;&#125;String[][] result = new String[total][args.length];for (int i = 0; i &lt; total; i++) &#123;for (int j = 0; j &lt; args.length; j++) &#123;result[i][j] = args[j][counter[j]];&#125;counterIndex = handle(counter, counterIndex, args);&#125;return result;&#125;private static int handle(int[] counter, int counterIndex, String[][] args) &#123;counter[counterIndex]++;if (counter[counterIndex] &gt;= args[counterIndex].length) &#123;counter[counterIndex] = 0;counterIndex--;if (counterIndex &gt;= 0) &#123;handle(counter, counterIndex, args);&#125;counterIndex = args.length - 1;&#125;return counterIndex;&#125;&#125; 原文作者： Maxith Zhou原文链接 http://maxith.com/2017/08/30/cartesian-product/]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Mockito和SpringTest进行单元测试]]></title>
    <url>%2F2017%2F11%2F13%2F%E4%BD%BF%E7%94%A8Mockito%E5%92%8CSpringTest%E8%BF%9B%E8%A1%8C%E5%8D%95%E5%85%83%E6%B5%8B%E8%AF%95%2F</url>
    <content type="text"><![CDATA[单元测试的作用以及重要性其实不用在这里不断的重复了.为了保证软件的可靠性,单元测试可能是最容易执行的一个可靠的手段了.一方面，程序员通过编写单元测试来验证自己程序的有效性,另外一方面,管理者通过持续自动的执行单元测试和分析单元测试的覆盖率等来确保软件本身的质量. JAVA生态圈里面说起单元测试一般都会使用JUnit或者TestNG.其中JUnit可能使用的更加频繁一些,JUnit4使用注解以及各种其他框架对它的支持,形成了一个完善的单元测试的生态圈. ##SpringTest单元测试现在的JAVA WEB项目中,起码一半以上的项目是使用了Spring的.因此,单纯的使用JUnit来进行单元测试并不是十分的好用,对于由Spring管理的Bean要进行单元测试,首先需要实例化Spring上下文,然后又需要手动的去注入依赖的Bean,比较麻烦.特别是对于有事务的单元测试,或数据库数据测试,单独使用JUnit几乎无法完成.所幸,SpringFramework也意识到了这点,于是推出了spring-test模块,他能完成Spring环境与Junit单元测试环境的对接.让我们只专注于单元测试本身进行书写,而由它来完成Spring容器的初始化、Bean的获取、数据库事务的管理、数据操作正确性检查等等。项目中引用spring-test要在项目中使用Spring测试框架非常的简单，在Maven中依赖spring-test即可，Maven会自动的完成依赖包的引用： 123456789101112131415 &lt;dependencies&gt;&lt;!--test--&gt;&lt;dependency&gt;&lt;groupId&gt;junit&lt;/groupId&gt;&lt;artifactId&gt;junit&lt;/artifactId&gt;&lt;version&gt;4.12&lt;/version&gt;&lt;scope&gt;test&lt;/scope&gt;&lt;/dependency&gt;&lt;dependency&gt;&lt;groupId&gt;org.springframework&lt;/groupId&gt;&lt;artifactId&gt;spring-test&lt;/artifactId&gt;&lt;version&gt;4.2.0.RELEASE&lt;/version&gt;&lt;scope&gt;test&lt;/scope&gt;&lt;/dependency&gt;&lt;/dependencies&gt; 创建单元测试类Spring-test以及Junit4都推荐使用注解的方式来进行配置。因此，我们要进行单元测试，需要的就是创建一个自己的测试类。然后在类上面进行少许的注解，表明我们要如何初始化spring，需要引用spring的哪些配置，然后再指明需要在单元测试类中注入哪些bean即可，比如： 12345678910111213141516171819202122232425262728 @RunWith(SpringJUnit4ClassRunner.class) // 整合Spring-Test与JUnit@ContextConfiguration(locations = &#123;"classpath:application_bean_commons.xml","classpath:application_bean_rmdbaccess.xml","classpath:application_pm_service_poolmanage.xml"&#125;) // 加载需要的配置配置@Transactional //事务回滚,便于重复测试public class TestPoolService&#123;@Autowiredprivate IPoolService poolService;@Beforepublic void setUp() &#123; //可以在每一次单元测试前准备执行一些东西&#125;@Testpublic void testAddPool() &#123;Pool pool = new Pool();pool.setPoolName("test_pool_001");pool.setDescription("test add pool");pool = poolService.addPool(pool);//进行结果的验证assertNotNull(pool.getId());assertNotNull(pool.getPoolCode());&#125;&#125; 从这个简单的例子当中，我们可以看到和单独使用JUnit有几个不一样的地方。 1. 首先使用了@RunWith(SpringJUnit4ClassRunner.class)来告诉JUnit，需要结合Spring-test来执行单元测试 2. 然后使用@ContextConfiguration注解来告诉Spring-test这个单元测试最小需要依赖的spring的上下文是什么，因为通常单元测试不需要引入所有的spring配置，因此我们可以在这里可选的读取几个需要的配置即可 3. 而后，我们可以在类上或者方法上使用@Transactional注解来标识某个或某些单元测试用例需要使用事务，并且可能会进行回滚，防止单元测试引起数据库脏数据。 4. 同时，我们可以在这个单元测试类中直接使用@Autowired以及@Qualifier注解直接注入经过Spring管理过的依赖Bean。而我们测试的主要目的也就是对这些bean进行单元测试。 5. 剩下的就和单独的使用JUnit进行单元测试是一样的了。 经过以上的处理，我们就可以进行spring框架下的单元测试了，而且基本上能满足80%以上的需求。更多更高级的用法，可以参考：JUnit官网以及Spring-test官网 ##基于数据库数据的单元测试数据准备其实这个和要讲的Spring-test没有必然的联系。但是在很多的情况下，我们都会对数据库访问层进行单元测试。那么，往往就涉及到了数据库的数据打桩。为了能实现单元测试的自动化和可重复化，我们可以把桩数据写入一个单元测试的SQL中，在每次执行单元测试的时候，先执行这个SQL，给数据库准备数据，然后再执行单元测试。而这一切，我们可以写一个测试的基类来进行处理（JDK1.8中允许给接口增加默认方法，因此，这个地方我们可以定义一个接口来实现这个功能）： 1234567891011121314151617181920212223242526272829303132333435363738 public interface DBTestBase &#123;// 接口的默认方法，对数据库进行打桩default void prepareRmdbData(SimpleRmdbDao simpleRmdbDao) &#123;String content = sqlForThisTest();if (content==null)&#123;return;&#125;String[] sqlLines = content.split(";");for (String sqlLine : sqlLines) &#123;simpleRmdbDao.executeDDL(sqlLine);&#125;&#125;// 解析和这个测试类相同名字的SQL，一行一句。default String sqlForThisTest() &#123;String sqlName = getClass().getSimpleName() + ".sql";try &#123;List&lt;String&gt; lines = IOUtils.readLines(getClass().getResourceAsStream(sqlName),"utf-8");/*去掉SQL中的注释*/Optional&lt;String&gt; optional = lines.parallelStream().filter(s -&gt; !(s.startsWith("-- ")&amp;&amp;s.endsWith(" --"))).reduce(String::concat);if (optional.isPresent())&#123;return optional.get();&#125;return null;&#125; catch (IOException e) &#123;e.printStackTrace();&#125;return null;&#125;&#125; 有了这个接口，我们在写单元测试类的时候，就可以直接实现这个接口，然后在@Before方法中调用prepareRmdbData方法来初始化数据库: 12345678910111213141516171819202122232425262728293031323334 @RunWith(SpringJUnit4ClassRunner.class) // 整合Spring-Test与JUnit@ContextConfiguration(locations = &#123;"classpath:application_bean_commons.xml","classpath:application_bean_rmdbaccess.xml","classpath:application_pm_service_poolmanage.xml"&#125;) // 加载需要的配置配置@Transactional //事务回滚,便于重复测试public class TestPoolService implements DBTestBase &#123;@Autowiredprivate IPoolService poolService;@Autowiredprivate SimpleRmdbDao simpleRmdbDao;@Beforepublic void setUp() &#123; //可以在每一次单元测试前准备执行一些东西 prepareRmdbData(simpleRmdbDao);&#125;@Testpublic void testDeletePool() &#123;//case1Pool pool = poolService.getPoolByCode("P-00003");assertNotNull(pool);poolService.deletePool("P-00003");pool = poolService.getPoolByCode("P-00003");assertNull(pool);&#125;&#125; 然后把SQL文件命名为TestPoolService.sql,放入test/resources中即可。Mock测试经过上面所说的JUnit+SpringTest,基本上可以满足80%的单元测试了。但是，由于现在的系统越来越复杂，相互之间的依赖越来越多。特别是微服务化以后的系统，往往一个模块的代码需要依赖几个其他模块的东西。因此，在做单元测试的时候，往往很难构造出需要的依赖。一个单元测试，我们只关心一个小的功能，但是为了这个小的功能能跑起来，可能需要依赖一堆其他的东西，这就导致了单元测试无法进行。所以，我们就需要再测试过程中引入Mock测试。所谓的Mock测试就是在测试过程中，对于一些不容易构造的、或者和这次单元测试无关但是上下文又有依赖的对象，用一个虚拟的对象（Mock对象）来模拟，以便单元测试能够进行。比如有一段代码的依赖为：当我们要进行单元测试的时候，就需要给A注入B和C,但是C又依赖了D，D又依赖了E。这就导致了，A的单元测试很难得进行。但是，当我们使用了Mock来进行模拟对象后，我们就可以把这种依赖解耦，只关心A本身的测试，它所依赖的B和C，全部使用Mock出来的对象，并且给MockB和MockC指定一个明确的行为。就像这样： 因此，当我们使用Mock后，对于那些难以构建的对象，就变成了个模拟对象，只需要提前的做Stubbing（桩）即可，所谓做桩数据，也就是告诉Mock对象，当与之交互时执行何种行为过程。比如当调用B对象的b()方法时，我们期望返回一个true，这就是一个设置桩数据的预期。 ##Mockito简单入门在JAVA中，Mock测试框架主要有Mockito,Jmock,EsayMock,PowerMock等等。其中Mockito最为方便和简单，用的人也最多。而PowerMock是对Mockito的一个增强,增加了对静态、final、私有方法的Mock，但是基本用法和Mockito大致相同。对因此，我们使用Mockito作为Mock的框架。项目中引用Mockito要在项目中使用Mockito非常的简单，只需要在项目的Maven中引入： 123456 &lt;dependency&gt;&lt;groupId&gt;org.mockito&lt;/groupId&gt;&lt;artifactId&gt;mockito-core&lt;/artifactId&gt;&lt;version&gt;2.1.0&lt;/version&gt;&lt;scope&gt;test&lt;/scope&gt;&lt;/dependency&gt; ###使用Mockito进行测试我们这里使用一个最简单的用户基本信息管理来做演示。这个功能有一个模型对象UserPO,一个数据库访问层UserDao,一个服务层UserService。UserPO 1234567891011121314151617181920212223242526272829303132 public class UserPO implements Serializable &#123;private Long id;private String name;private Integer age;public Long getId() &#123;return id;&#125;public void setId(Long id) &#123;this.id = id;&#125;public String getName() &#123;return name;&#125;public void setName(String name) &#123;this.name = name;&#125;public Integer getAge() &#123;return age;&#125;public void setAge(Integer age) &#123;this.age = age;&#125;&#125; UserDao 1234567 public interface IUserDao &#123;public Boolean updateUser(UserPO userPO);public UserPO getUserById(Long id);&#125; UserService 123456789101112131415161718192021 public class UserService &#123;private IUserDao userDao;public void setUserDao(IUserDao userDao) &#123;this.userDao = userDao;&#125;public boolean updateUserName(Long userId,String name)&#123;UserPO userPO = userDao.getUserById(userId);if (userPO==null)&#123;return false;&#125;userPO.setName(name);return userDao.updateUser(userPO);&#125;&#125; ###创建单元测试类当我们准备好上面的例子后，就可以开始创建单元测试类了。在这里，我们假设IUserDao是个很复杂的访问集群数据库的对象，并且这个类已经经过完整的测试保证是正确了的。而我们现在只需要单元测试UserService#updateUserName这个方法。因此，就需要对IUserDao进行Mock并打桩。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970 public class UserServiceTest &#123;private UserService userService;@Mockprivate IUserDao userDao;@Beforepublic void setUp() &#123;//对注解了@Mock的对象进行模拟MockitoAnnotations.initMocks(this);//构造被测试对象userService = new UserService();userService.setUserDao(userDao);//数据打桩。 当调用 userDao.getUserById(1L)时，返回一个UserPOMockito.when(userDao.getUserById(1L)).thenReturn(new UserPO(1L,"user1",20));// 当调用 userDao.getUserById(2L)时，返回一个null，表示用户不存在Mockito.when(userDao.getUserById(2L)).thenReturn(null);// 当调用userDao.updateUser(userPO)的时候，返回一个trueMockito.when(userDao.updateUser(Mockito.any())).thenReturn(true);&#125;@Testpublic void testUpdateUserNameSuccess() &#123;/*成功的情况*//*测试这个被测试对象的方法*/boolean updated = userService.updateUserName(1L,"user_new");//验证结果Assert.assertTrue(updated);//验证userDao的getUserById(1L)这个方法是否被调用过Mockito.verify(userDao).getUserById(1L);//构造参数捕获器，用于捕获方法参数进行验证ArgumentCaptor&lt;UserPO&gt; personCaptor = ArgumentCaptor.forClass( UserPO.class );//验证updateUser方法是否被调用过，并且捕获入参Mockito.verify(userDao).updateUser(personCaptor.capture());//返回捕获的参数。UserPO updatedPerson = personCaptor.getValue();//判断是否已经被修改过了Assert.assertEquals("user_new", updatedPerson.getName());//多余方法调用验证，保证这个测试用例中所有被Mock的对象的相关方法都已经被Verify过了Mockito.verifyNoMoreInteractions(userDao);&#125;@Testpublic void testUpdateUserNameFailed() &#123;/*测试这个被测试对象的方法*/boolean updated = userService.updateUserName(2L,"user_new");//验证结果Assert.assertFalse(updated);//验证userDao的getUserById(1L)这个方法是否被调用过Mockito.verify(userDao).getUserById(2L);//多余方法调用验证，保证这个测试用例中所有被Mock的对象的相关方法都已经被Verify过了Mockito.verifyZeroInteractions(userDao);//多余方法调用验证，保证这个测试用例中所有被Mock的对象的相关方法都已经被Verify过了Mockito.verifyNoMoreInteractions(userDao);&#125;&#125; 上面的代码就是一个完整的单元测试类，有两个用例，分别验证当用户存在能修改名字的情况以及用户不存在修改名字失败的情况。我们从头来分析一下这个单元测试类。 ###标明需要Mock的对象程序一来，先定义了被测试的对象实例userService以及需要被模拟的IUserDao对象。需要注意的是，我们在userDao成员变量上增加了一个@Mock注解。这个注解的作用就是告诉Mockito，这个对象是需要被Mock的。接着，我们创建了一个setUp()方法，并使用了JUnit的注解@Before，用于在执行单元测试前执行一些代码，我们在这里需要对Mock的对象进行打桩。MockitoAnnotations.initMocks(this);这句话就是对所有标注了@Mock注解的对象进行模拟。当然，我们也可以不使用注解，而直接使用代码的方式手动的初始化Mock的对象： 12345 //对注解了@Mock的对象进行模拟// MockitoAnnotations.initMocks(this);//使用手动的方式进行MockuserDao = Mockito.mock(IUserDao.class); 接着就是指定userDao的行为也就是桩了。这也是Mockito最常用最核心的方法了。 1234567 //数据打桩。 当调用 userDao.getUserById(1L)时，返回一个UserPOMockito.when(userDao.getUserById(1L)).thenReturn(new UserPO(1L,"user1",20));// 当调用 userDao.getUserById(2L)时，返回一个null，表示用户不存在Mockito.when(userDao.getUserById(2L)).thenReturn(null);// 当调用userDao.updateUser(userPO)的时候，返回一个trueMockito.when(userDao.updateUser(Mockito.any())).thenReturn(true); Mockito最基本的用法就是调用 when以及thenReturn方法了。他们的作用就是指定当我们调用被代理的对象的某一个方法以及参数的时候，返回什么值。 比如第一句的Mockito.when(userDao.getUserById(1L)).thenReturn(new UserPO(1L,”user1”,20));就表明，当我调用userDao.getUserById(1L)的时候，这个方法返回new UserPO(1L,”user1”,20)这个实例。 当我们需要返回NULL的时候，也非常的简单，直接写成thenReturn(null)即可。 如果我们不关心调用的参数的入参，那么Mockito提供了几个方法来表示:any()、any(Class type)、anyBoolean()、anyByte()、anyChar()、anyInt()、anyLong()、anyFloat()、anyDouble()、anyShort()、anyString()、anyList()、anyListOf(Class clazz)、anySet()、anyMap()等等 相反，Mockito还提供了很强大的入参过滤，用于指定只对某一些入参的调用进行Mock。比如：正则表达式Mockito.matches(“.*User$”))、开头结尾验证endsWith(String suffix) startsWith(String prefix)、判空验证isNotNull() isNull() 甚至，我们还可以自定义入参匹配：argThat(ArgumentMatcher matcher)。ArgumentMatcher只有一个方法boolean matches(T argument);传入入参，返回一个boolean表示是否匹配。在JDK1.8中，我们可以使用lambda表达式来自定义入参匹配，比如：1Mockito.argThat(argument -&gt; argument instanceof UserPO); 除了我们期望调用一个方法后返回一个值外，有些时候，我们可能期望他抛出一个异常。这个时候，我们可以调用thenThrow(Throwable… throwables); 用来抛出异常，这个方法有三个重载: 123* thenThrow(Throwable... throwables): 直接指定抛出的异常实例* thenThrow(Class&lt;? extends Throwable&gt; throwableType): 指定抛出异常的类型，执行的时候动态的实例化一个异常实例* thenThrow(Class&lt;? extends Throwable&gt; toBeThrown, Class&lt;? extends Throwable&gt;... nextToBeThrown): 多次调用，依次抛出异常 //当调用userDao的更新时，如果传入的用户的名字是admin，那么就不允许修改，直接抛出异常 1Mockito.when(userDao.updateUser(Mockito.argThat(argument -&gt; argument.getName().equals("admin")))).thenThrow(IllegalArgumentException.class); 此外，Mockito还提供了两个表示行为的方法：thenAnswer(Answer&lt;?&gt; answer);、thenCallRealMethod();,分别表示自定义处理调用后的行为，以及调用真实的方法。这两个方法在有些测试用例中还是很有用的。 对于同一个方法，Mockito可以是顺序与次数关心的。也就是说可以实现同一个方法，第一次调用返回一个值，第二次调用返回一个值，甚至第三次调用抛出异常等等。只需要连续的调用thenXXXX即可。 最后，还有一个需要说明的就是如果为一个返回为Void的方法设置桩数据。上面的方法都是表示的是有返回值的方法，而由于一个方法没有返回值，因此我们不能调用when方法(编译器不允许)。因此，对于无返回值的方法，Mockito提供了一些列的doXXXXX方法，比如：doAnswer(Answer answer)、doNothing()、doReturn(Object toBeReturned)、doThrow(Class&lt;? extends Throwable&gt; toBeThrown)、doCallRealMethod()。他们的使用方法其实和上面的thenXXXX是一样的，但是when方法传入的是Mock的对象：12345 /*对void的方法设置模拟*/Mockito.doAnswer(invocationOnMock -&gt; &#123;System.out.println("进入了Mock");return null;&#125;).when(fileRecordDao).insert(Mockito.any()); ###验证Mock对象的调用其实，一个最简单的Mock单元测试到这里已经算是完成了。我们已经验证了userService中的方法的正确性。但是，在复杂的方法调用堆栈中，往往可能出现结果正确，但是过程不正确的情况。比如，updateUserName方法返回false是有两种可能的，一种可能是用户没有找到，还有一种可能就是userDao.updateUser(userPO)返回false。因此，如果我们只是使用Assert.assertFalse(updated);来验证结果，可能就会忽略某些错误。Mockito同时提供了一些列的方法来对调用过程中的Mock对象的方法调用进行跟踪。我们可以对这些调用的过程进行断言验证，保证单元测试的结果与过程都是符合我们预期的。 123456789101112131415161718192021222324252627 @Testpublic void testUpdateUserNameSuccess() &#123;/*成功的情况*//*测试这个被测试对象的方法*/boolean updated = userService.updateUserName(1L,"user_new");//验证结果Assert.assertTrue(updated);//验证userDao的getUserById(1L)这个方法是否被调用过Mockito.verify(userDao).getUserById(1L);//构造参数捕获器，用于捕获方法参数进行验证ArgumentCaptor&lt;UserPO&gt; personCaptor = ArgumentCaptor.forClass( UserPO.class );//验证updateUser方法是否被调用过，并且捕获入参Mockito.verify(userDao).updateUser(personCaptor.capture());//返回捕获的参数。UserPO updatedPerson = personCaptor.getValue();//判断是否已经被修改过了Assert.assertEquals("user_new", updatedPerson.getName());//多余方法调用验证，保证这个测试用例中所有被Mock的对象的相关方法都已经被Verify过了Mockito.verifyNoMoreInteractions(userDao);&#125; Mockito.verify(userDao).getUserById(1L);方法即验证了getUserById(1L)这个方法是否被调用过，如果没有被调用过(包括入参要一致)，就会抛出异常。 除了最简单的verify(T mock)方法外，还提供了verify(T mock, VerificationMode mode)方法。第二个参数有很多默认的实现，用于满足不同的需求。比如：Mockito.verify(userDao,Mockito.times(1)).getUserById(1L); 表示调用第一次 是传入的getUserById(1L);，Mockito.verify(userDao,Mockito.times(2)).getUserById(2L);表示调用第二次是传入的getUserById(2L);，如果测试用例的调用顺序与参数不满足的话，就会报错。 除了times函数外，还提供了after、atLeast、only、atMost、timeout等等。 verifyZeroInteractions和verifyNoMoreInteractions这两个方法的实现其实是一样的，只是名字不一样，作用就是验证被Mock的对象的所有被调用的方法是否都被Verify过了。这样就能保证调用没有被遗漏。当有方法被调用了，但是我们在测试用例中没有verify的话，那么调用这两个方法就会抛异常。##Mockito与SpringTest整合经过前面的讲解，Mockito的基本用法基本上应该都了解了。那么现在就需要整合Mockito和SpringTest了。其实这两者的整合也非常的简单。和他们单独使用的时候并没有什么区别。1234567891011121314151617181920212223242526272829303132333435363738394041 @RunWith(SpringJUnit4ClassRunner.class) // 整合Spring-Test与JUnit@ContextConfiguration(locations = &#123;"classpath:application_bean_commons.xml","classpath:application_bean_rmdbaccess.xml","classpath:application_pm_service_poolmanage.xml"&#125;) // 加载需要的配置配置@Transactional //事务回滚,便于重复测试public class TestPoolService&#123;@Autowiredprivate IPoolService poolService;@Mockprivate IPoolDao poolDao;@Beforepublic void setUp() &#123; //可以在每一次单元测试前准备执行一些东西 MockitoAnnotations.initMocks(this);//把Spring上下文注入的对象给替换掉 ReflectionTestUtils.setField(AopTargetUtils.getTarget(poolService), ”poolDao“,poolDao);/*对void的方法设置模拟*/Mockito.doAnswer(invocationOnMock -&gt; &#123;System.out.println("进入了Mock");return null;&#125;).when(poolDao).insert(Mockito.any());&#125;@Testpublic void testAddPool() &#123;Pool pool = new Pool();pool.setPoolName("test_pool_001");pool.setDescription("test add pool");pool = poolService.addPool(pool);//进行结果的验证assertNotNull(pool.getId());assertNotNull(pool.getPoolCode());&#125;&#125; 与单独使用Mockito相比，最大的不同其实就是在setUp()方法中调用的ReflectionTestUtils.setField(AopTargetUtils.getTarget(poolService), ”poolDao“,poolDao);这个方法。ReflectionTestUtils是Spring-test提供的一个用于反射处理测试类的工具，通过这个，我们可以替换某一个被spring所管理的bean的成员变量。把他换成我们Mock出来的模拟对象。当然这又引出了一个问题，就是如果依赖的对象的依赖对象需要被Mock，那么手动的不断重复的找需要被Mock的成员变量非常的麻烦。因此，我们可以写一个AbstractTestExecutionListener监听器，当注入依赖的时候，找到被Mock的变量，以及需要被注入的变量，然后做关系的依赖。这样就能自动的对成员变量做替换了。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091 public class MockitoDependencyInjectionTestExecutionListener extends DependencyInjectionTestExecutionListener &#123;private static final Map&lt;Class,Object&gt; mockObject = new HashMap&lt;&gt;();@Overrideprotected void injectDependencies(final TestContext testContext) throws Exception &#123;super.injectDependencies(testContext);init(testContext);&#125;private void injectMock(Object bean) throws Exception &#123;Field[] fields;/*找到所有的测试用例的字段*/if (AopUtils.isAopProxy(bean))&#123;// 如果是代理的话，找到真正的对象if(AopUtils.isJdkDynamicProxy(bean)) &#123;Class targetClass = AopTargetUtils.getTarget(bean).getClass();if (targetClass == null) &#123;// 可能是远程实现return;&#125;fields = targetClass.getDeclaredFields();&#125; else &#123; //cglib/*CGLIB的代理 不支持*/return;&#125;&#125;else &#123;fields = bean.getClass().getDeclaredFields();&#125;List&lt;Field&gt; injectFields = new ArrayList&lt;&gt;();/*判断字段上的注解*/for (Field field : fields) &#123;Annotation[] annotations = field.getAnnotations();for (Annotation antt : annotations) &#123;/*如果是Mock字段的,就直接注入Mock的对象*/if (antt instanceof org.mockito.Mock) &#123;// 注入mock实例Object mockObj = mock(field.getType());mockObject.put(field.getType(),mockObj);field.setAccessible(true);field.set(bean, mockObj);&#125; else if (antt instanceof Autowired) &#123;/*需要把所有标注为autowired的找到*/injectFields.add(field);&#125;&#125;&#125;/*访问每一个被注入的实例*/for (Field field : injectFields) &#123;field.setAccessible(true);/*找到每一个字段的值*/Object object = field.get(bean);Class targetClass = field.getType();if (!replaceInstance(targetClass,bean,field.getName()))&#123;//如果没有被mock过.那么这个字段需要再一次的做递归injectMock(object);&#125;&#125;&#125;private boolean replaceInstance(Class targetClass, Object bean, String fieldName) throws Exception &#123;boolean beMocked = false;for (Map.Entry&lt;Class, Object&gt; classObjectEntry : mockObject.entrySet()) &#123;Class type = classObjectEntry.getKey();if (type.isAssignableFrom(targetClass))&#123;//如果这个字段是被mock了的对象.那么就使用这个mock的对象来替换ReflectionTestUtils.setField(AopTargetUtils.getTarget(bean), fieldName, classObjectEntry.getValue());beMocked = true;break;&#125;&#125;return beMocked;&#125;private void init(final TestContext testContext) throws Exception &#123;Object bean = testContext.getTestInstance();injectMock(bean);&#125;&#125; 以上代码即为Mock初始化的监听器。它会查询这个测试用例的所有的成员变量。找到被标记为@Mock的变量，然后模拟出来。而后，又找到所有被标注为@Autowired的成员变量，判断变量类型是否是需要被Mock的。当需要使用这个监听器的时候，只需要增加一个注解@TestExecutionListeners即可： 12345678910111213141516171819202122232425262728293031323334353637 @RunWith(SpringJUnit4ClassRunner.class) // 整合Spring-Test与JUnit/*如果要加事物,那么当手动设置TestExecutionListeners的时候,需要把TransactionalTestExecutionListener这个也加上*/@TestExecutionListeners(&#123; MockitoDependencyInjectionTestExecutionListener.class, TransactionalTestExecutionListener.class &#125;)@ContextConfiguration(locations = &#123;"classpath:application_bean_commons.xml","classpath:application_bean_rmdbaccess.xml","classpath:application_pm_service_poolmanage.xml"&#125;) // 加载需要的配置配置@Transactional //事务回滚,便于重复测试public class TestPoolService&#123;@Autowiredprivate IPoolService poolService;@Mockprivate IPoolDao poolDao;@Beforepublic void setUp() &#123; /*对void的方法设置模拟*/Mockito.doAnswer(invocationOnMock -&gt; &#123;System.out.println("进入了Mock");return null;&#125;).when(poolDao).insert(Mockito.any());&#125;@Testpublic void testAddPool() &#123;Pool pool = new Pool();pool.setPoolName("test_pool_001");pool.setDescription("test add pool");pool = poolService.addPool(pool);//进行结果的验证assertNotNull(pool.getId());assertNotNull(pool.getPoolCode());&#125;&#125;]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>单元测试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker容器访问外部网络]]></title>
    <url>%2F2017%2F10%2F19%2FDocker%E5%AE%B9%E5%99%A8%E8%AE%BF%E9%97%AE%E5%A4%96%E9%83%A8%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[最近工作中部署一个项目，在项目内部需要访问外网。给某云上传文件，但是一直报unknown host，无法解析域名，然后找了好久原因，下面废话不多说，来一起看看详细的解决方法。 ##解决方法Linux系统默认没有打开IP转发功能，要确认IP转发功能的状态，可以查看/proc文件系统，使用下面命令： 12cat /proc/sys/net/ipv4/ip_forward0 如果上述文件中的值为0,说明禁止进行IP转发；如果是1,则说明IP转发功能已经打开,要想打开IP转发功能，可以直接修改上述文件： 1echo 1 &gt; /proc/sys/net/ipv4/ip_forward 把文件的内容由0修改为1。禁用IP转发则把1改为0。上面的命令并没有保存对IP转发配置的更改，下次系统启动时仍会使用原来的值，要想永久修改IP转发，需要修改/etc/sysctl.conf文件，修改下面一行的值： 1net.ipv4.ip_forward = 1 修改后可以重启系统来使修改生效，也可以执行下面的命令来使修改生效： 1sysctl -p /etc/sysctl.conf 进行了上面的配置后，IP转发功能就永久开启了]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker开启远程API]]></title>
    <url>%2F2017%2F10%2F19%2FDocker%E5%BC%80%E5%90%AF%E8%BF%9C%E7%A8%8BAPI%2F</url>
    <content type="text"><![CDATA[有时需要远程调用DockerAPI进行容器操作，比如Jenkins，本文讲述如何开启Docker远程API。 修改配置文件 CentOS6:1/etc/sysconfig/docker 添加一行: 1DOCKER_OPTS='-H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock' CentOS7: 1/usr/lib/systemd/system/docker.service 修改一行: 1ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock 重启Docker CentOS6: 1service docker restart CentOS7:12systemctl daemon-reloadsystemctl restart docker.service 测试在本机 1curl http://127.0.0.1:2375/info 在其他机器上： 1curl http://hostanme:2375/info 此时本地client可以继续通过Unix sock与docker daemon通行例如：docker info 命令继续有效。]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[细思极恐-你真的会写java吗?]]></title>
    <url>%2F2017%2F10%2F13%2F%E7%BB%86%E6%80%9D%E6%9E%81%E6%81%90-%E4%BD%A0%E7%9C%9F%E7%9A%84%E4%BC%9A%E5%86%99java%E5%90%97%2F</url>
    <content type="text"><![CDATA[本文转载自一位优秀程序猿，笔者是一个务实的程序员，故本文绝非扯淡文章，文中内容都是干货，望读者看后，能有所收获。 文章核心其实，本不想把标题写的那么恐怖，只是发现很多人干了几年java以后，都自认为是一个不错的java程序员了，可以拿着上万的工资都处宣扬自己了，写这篇文章的目的并不是嘲讽和我一样做java的同行们，只是希望读者看到此骗文章后，可以和我一样，心平气和的争取做一个优秀的程序员。 讲述方向由于一直从事移动互联网相关工作，java开发中经常和移动端打交道或者做一些后端的工作，所以本篇文章更可能涉及和移动端的交互或者与后端的交互方式，笔者希望以自身的一些学习经验或者开发经验，可以带动认真阅读本篇文章的读者们，让大家对java有一个更好的态度去学习它，它不只是一个赚钱的工具而已。笔者身边有很多与笔者年龄相仿或年龄更大的朋友或同事，经常有人问我：“你现在还在学习吗？我觉得没什么好学的，这些东西都差不多”，我总是回答只要有时间，我就要看一会书，这个时候，大家都会露出一副不屑的眼神或笑容。其实，非常能理解身边朋友或同事的看法，以目前状态来讲，大多都是工作至少5年的程序员了，对于公司大大小小的业务需要，以目前的知识储备来讲，都可以轻松应对，“没有什么好学的”其实这句话没有多大的问题，但是，如果你对编程还有一点点兴趣，只是不知道如何努力或改进，希望本篇文章可以帮到你。 技术点本文不是一个吹嘘的文章，不会讲很多高深的架构，相反，会讲解很多基础的问题和写法问题，如果读者自认为基础问题和写法问题都是不是问题，那请忽略这篇文章，节省出时间去做一些有意义的事情。 开发工具不知道有多少”老”程序员还在使用eclipse，这些程序员们要不就是因循守旧，要不就是根本就不知道其他好的开发工具的存在，eclipse吃内存卡顿的现象以及各种偶然莫名异常的出现，都告知我们是时候寻找新的开发工具了。 更换IDE根本就不想多解释要换什么样的IDE，如果你想成为一个优秀的java程序员，请更换intellij idea. 使用idea的好处，请搜索谷歌。 别告诉我快捷键不好用更换IDE不在我本文的重点内容中，所以不下想用太多的篇幅去写为什么更换IDE，请谷歌。在这里，我只能告诉你，更换IDE只为了更好、更快的写好java代码。原因略。别告诉我快捷键不好用，请尝试新事物。 beanbean使我们使用最多的模型之一，我将以大篇幅去讲解bean，希望读者好好体会。 domain包名根据很多java程序员的”经验”来看，一个数据库表则对应着一个domain对象，所以很多程序员在写代码时，包名则使用：com.xxx.domain ，这样写好像已经成为了行业的一种约束，数据库映射对象就应该是domain。但是你错了，domain是一个领域对象，往往我们再做传统java软件web开发中，这些domain都是贫血模型，是没有行为的，或是没有足够的领域模型的行为的，所以，以这个理论来讲，这些domain都应该是一个普通的entity对象，并非领域对象，所以请把包名改为:com.xxx.entity。如果你还不理解我说的话，请看一下Vaughn Vernon出的一本叫做《IMPLEMENTING DOMAIN-DRIVEN DESIGN》(实现领域驱动设计)这本书，书中讲解了贫血模型与领域模型的区别，相信你会受益匪浅。 DTO数据传输我们应该使用DTO对象作为传输对象，这是我们所约定的，因为很长时间我一直都在做移动端api设计的工作，有很多人告诉我，他们认为只有给手机端传输数据的时候(input or output)，这些对象成为DTO对象。请注意！这种理解是错误的，只要是用于网络传输的对象，我们都认为他们可以当做是DTO对象，比如电商平台中，用户进行下单，下单后的数据，订单会发到OMS 或者 ERP系统，这些对接的返回值以及入参也叫DTO对象。我们约定某对象如果是DTO对象，就将名称改为XXDTO,比如订单下发OMS：OMSOrderInputDTO。 DTO转化正如我们所知，DTO为系统与外界交互的模型对象，那么肯定会有一个步骤是将DTO对象转化为BO对象或者是普通的entity对象，让service层去处理。 场景比如添加会员操作，由于用于演示，我只考虑用户的一些简单数据，当后台管理员点击添加用户时，只需要传过来用户的姓名和年龄就可以了，后端接受到数据后，将添加创建时间和更新时间和默认密码三个字段，然后保存数据库。 12345678910111213141516@RequestMapping("/v1/api/user")@RestControllerpublic class UserApi &#123;@Autowiredprivate UserService userService;@PostMappingpublic User addUser(UserInputDTO userInputDTO)&#123;User user = new User();user.setUsername(userInputDTO.getUsername());user.setAge(userInputDTO.getAge());return userService.addUser(user);&#125;&#125; 我们只关注一下上述代码中的转化代码，其他内容请忽略: 123User user = new User();user.setUsername(userInputDTO.getUsername());user.setAge(userInputDTO.getAge()); 请使用工具上边的代码，从逻辑上讲，是没有问题的，只是这种写法让我很厌烦，例子中只有两个字段，如果有20个字段，我们要如何做呢？ 一个一个进行set数据吗？当然，如果你这么做了，肯定不会有什么问题，但是，这肯定不是一个最优的做法。网上有很多工具，支持浅拷贝或深拷贝的Utils. 举个例子，我们可以使用org.springframework.beans.BeanUtils#copyProperties对代码进行重构和优化: 1234567@PostMappingpublic User addUser(UserInputDTO userInputDTO)&#123;User user = new User();BeanUtils.copyProperties(userInputDTO,user);return userService.addUser(user);&#125; BeanUtils.copyProperties是一个浅拷贝方法，复制属性时，我们只需要把DTO对象和要转化的对象两个的属性值设置为一样的名称，并且保证一样的类型就可以了。如果你在做DTO转化的时候一直使用set进行属性赋值，那么请尝试这种方式简化代码，让代码更加清晰! 转化的语义上边的转化过程，读者看后肯定觉得优雅很多，但是我们再写java代码时，更多的需要考虑语义的操作，再看上边的代码: 12User user = new User();BeanUtils.copyProperties(userInputDTO,user); 虽然这段代码很好的简化和优化了代码，但是他的语义是有问题的，我们需要提现一个转化过程才好,所以代码改成如下: 12345678910111213@PostMappingpublic User addUser(UserInputDTO userInputDTO)&#123;User user = convertFor(userInputDTO);return userService.addUser(user);&#125;private User convertFor(UserInputDTO userInputDTO)&#123;User user = new User();BeanUtils.copyProperties(userInputDTO,user);return user;&#125; 这是一个更好的语义写法，虽然他麻烦了些，但是可读性大大增加了，在写代码时，我们应该尽量把语义层次差不多的放到一个方法中，比如: 12User user = convertFor(userInputDTO);return userService.addUser(user); 这两段代码都没有暴露实现，都是在讲如何在同一个方法中，做一组相同层次的语义操作，而不是暴露具体的实现。如上所述，是一种重构方式，读者可以参考Martin Fowler的《Refactoring Imporving the Design of Existing Code》(重构 改善既有代码的设计) 这本书中的Extract Method重构方式。 抽象接口定义当实际工作中，完成了几个api的DTO转化时，我们会发现，这样的操作有很多很多，那么应该定义好一个接口，让所有这样的操作都有规则的进行。如果接口被定义以后，那么convertFor这个方法的语义将产生变化，他将是一个实现类。看一下抽象后的接口: 123public interface DTOConvert&lt;S,T&gt; &#123;T convert(S s);&#125; 虽然这个接口很简单，但是这里告诉我们一个事情，要去使用泛型，如果你是一个优秀的java程序员，请为你想做的抽象接口，做好泛型吧。我们再来看接口实现: 12345678public class UserInputDTOConvert implements DTOConvert &#123;@Overridepublic User convert(UserInputDTO userInputDTO) &#123;User user = new User();BeanUtils.copyProperties(userInputDTO,user);return user;&#125;&#125; 我们这样重构后，我们发现现在的代码是如此的简洁，并且那么的规范: 1234567891011121314@RequestMapping("/v1/api/user")@RestControllerpublic class UserApi &#123;@Autowiredprivate UserService userService;@PostMappingpublic User addUser(UserInputDTO userInputDTO)&#123;User user = new UserInputDTOConvert().convert(userInputDTO);return userService.addUser(user);&#125;&#125; review code如果你是一个优秀的java程序员，我相信你应该和我一样，已经数次重复review过自己的代码很多次了。我们再看这个保存用户的例子，你将发现，api中返回值是有些问题的，问题就在于不应该直接返回User实体，因为如果这样的话，就暴露了太多实体相关的信息，这样的返回值是不安全的，所以我们更应该返回一个DTO对象，我们可称它为UserOutputDTO: 1234567@PostMappingpublic UserOutputDTO addUser(UserInputDTO userInputDTO)&#123;User user = new UserInputDTOConvert().convert(userInputDTO);User saveUserResult = userService.addUser(user);UserOutputDTO result = new UserOutDTOConvert().convertToUser(saveUserResult);return result;&#125; 这样你的api才更健全。不知道在看完这段代码之后，读者有是否发现还有其他问题的存在，作为一个优秀的java程序员，请看一下这段我们刚刚抽象完的代码:User user = new UserInputDTOConvert().convert(userInputDTO);你会发现，new这样一个DTO转化对象是没有必要的，而且每一个转化对象都是由在遇到DTO转化的时候才会出现，那我们应该考虑一下，是否可以将这个类和DTO进行聚合呢，看一下我的聚合结果: 123456789101112131415161718192021222324252627282930313233343536public class UserInputDTO &#123;private String username;private int age;public String getUsername() &#123;return username;&#125;public void setUsername(String username) &#123;this.username = username;&#125;public int getAge() &#123;return age;&#125;public void setAge(int age) &#123;this.age = age;&#125;public User convertToUser()&#123;UserInputDTOConvert userInputDTOConvert = new UserInputDTOConvert();User convert = userInputDTOConvert.convert(this);return convert;&#125;private static class UserInputDTOConvert implements DTOConvert&lt;UserInputDTO,User&gt; &#123;@Overridepublic User convert(UserInputDTO userInputDTO) &#123;User user = new User();BeanUtils.copyProperties(userInputDTO,user);return user;&#125;&#125;&#125; 然后api中的转化则由: 12User user = new UserInputDTOConvert().convert(userInputDTO);User saveUserResult = userService.addUser(user); 变成了: 12User user = userInputDTO.convertToUser();User saveUserResult = userService.addUser(user); 我们再DTO对象中添加了转化的行为，我相信这样的操作可以让代码的可读性变得更强，并且是符合语义的。 再查工具类再来看DTO内部转化的代码，它实现了我们自己定义的DTOConvert接口，但是这样真的就没有问题，不需要再思考了吗？我觉得并不是，对于Convert这种转化语义来讲，很多工具类中都有这样的定义，这中Convert并不是业务级别上的接口定义，它只是用于普通bean之间转化属性值的普通意义上的接口定义，所以我们应该更多的去读其他含有Convert转化语义的代码。我仔细阅读了一下GUAVA的源码，发现了com.google.common.base.Convert这样的定义: 12345public abstract class Converter&lt;A, B&gt; implements Function&lt;A, B&gt; &#123;protected abstract B doForward(A a);protected abstract A doBackward(B b);//其他略&#125; 从源码可以了解到，GUAVA中的Convert可以完成正向转化和逆向转化，继续修改我们DTO中转化的这段代码: 12345678private static class UserInputDTOConvert implements DTOConvert&lt;UserInputDTO,User&gt; &#123;@Overridepublic User convert(UserInputDTO userInputDTO) &#123;User user = new User();BeanUtils.copyProperties(userInputDTO,user);return user;&#125;&#125; 修改后: 123456789101112131415private static class UserInputDTOConvert extends Converter&lt;UserInputDTO, User&gt; &#123;@Overrideprotected User doForward(UserInputDTO userInputDTO) &#123;User user = new User();BeanUtils.copyProperties(userInputDTO,user);return user;&#125;@Overrideprotected UserInputDTO doBackward(User user) &#123;UserInputDTO userInputDTO = new UserInputDTO();BeanUtils.copyProperties(user,userInputDTO);return userInputDTO;&#125;&#125; 看了这部分代码以后，你可能会问，那逆向转化会有什么用呢？其实我们有很多小的业务需求中，入参和出参是一样的，那么我们变可以轻松的进行转化，我将上边所提到的UserInputDTO和UserOutputDTO都转成UserDTO展示给大家:DTO： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950public class UserDTO &#123;private String username;private int age;public String getUsername() &#123;return username;&#125;public void setUsername(String username) &#123;this.username = username;&#125;public int getAge() &#123;return age;&#125;public void setAge(int age) &#123;this.age = age;&#125;public User convertToUser()&#123;UserDTOConvert userDTOConvert = new UserDTOConvert();User convert = userDTOConvert.convert(this);return convert;&#125;public UserDTO convertFor(User user)&#123;UserDTOConvert userDTOConvert = new UserDTOConvert();UserDTO convert = userDTOConvert.reverse().convert(user);return convert;&#125;private static class UserDTOConvert extends Converter&lt;UserDTO, User&gt; &#123;@Overrideprotected User doForward(UserDTO userDTO) &#123;User user = new User();BeanUtils.copyProperties(userDTO,user);return user;&#125;@Overrideprotected UserDTO doBackward(User user) &#123;UserDTO userDTO = new UserDTO();BeanUtils.copyProperties(user,userDTO);return userDTO;&#125;&#125;&#125; api: 1234567@PostMappingpublic UserDTO addUser(UserDTO userDTO)&#123;User user = userDTO.convertToUser();User saveResultUser = userService.addUser(user);UserDTO result = userDTO.convertFor(saveResultUser);return result;&#125; 当然，上述只是表明了转化方向的正向或逆向，很多业务需求的出参和入参的DTO对象是不同的，那么你需要更明显的告诉程序：逆向是无法调用的: 12345678910111213private static class UserDTOConvert extends Converter&lt;UserDTO, User&gt; &#123;@Overrideprotected User doForward(UserDTO userDTO) &#123;User user = new User();BeanUtils.copyProperties(userDTO,user);return user;&#125;@Overrideprotected UserDTO doBackward(User user) &#123;throw new AssertionError("不支持逆向转化方法!");&#125;&#125; 看一下doBackward方法，直接抛出了一个断言异常，而不是业务异常，这段代码告诉代码的调用者，这个方法不是准你调用的，如果你调用，我就”断言”你调用错误了。关于异常处理的更详细介绍，可以参考我之前的文章:如何优雅的设计java异常 ，应该可以帮你更好的理解异常。 bean的验证如果你认为我上边写的那个添加用户api写的已经非常完美了，那只能说明你还不是一个优秀的程序员。我们应该保证任何数据的入参到方法体内都是合法的。为什么要验证很多人会告诉我，如果这些api是提供给前端进行调用的，前端都会进行验证啊，你为什还要验证？其实答案是这样的，我从不相信任何调用我api或者方法的人，比如前端验证失败了，或者某些人通过一些特殊的渠道(比如Charles进行抓包),直接将数据传入到我的api，那我仍然进行正常的业务逻辑处理，那么就有可能产生脏数据！“对于脏数据的产生一定是致命”，这句话希望大家牢记在心，再小的脏数据也有可能让你找几个通宵！ jsr 303验证hibernate提供的jsr 303实现，我觉得目前仍然是很优秀的，具体如何使用，我不想讲，因为谷歌上你可以搜索出很多答案!再以上班的api实例进行说明，我们现在对DTO数据进行检查: 1234567public class UserDTO &#123;@NotNullprivate String username;@NotNullprivate int age;//其他代码略&#125; api验证: 1234567@PostMappingpublic UserDTO addUser(@Valid UserDTO userDTO)&#123;User user = userDTO.convertToUser();User saveResultUser = userService.addUser(user);UserDTO result = userDTO.convertFor(saveResultUser);return result;&#125; 我们需要将验证结果传给前端，这种异常应该转化为一个api异常(带有错误码的异常)。 1234567891011121314@PostMappingpublic UserDTO addUser(@Valid UserDTO userDTO, BindingResult bindingResult)&#123;checkDTOParams(bindingResult);User user = userDTO.convertToUser();User saveResultUser = userService.addUser(user);UserDTO result = userDTO.convertFor(saveResultUser);return result;&#125;private void checkDTOParams(BindingResult bindingResult)&#123;if(bindingResult.hasErrors())&#123;//throw new 带验证码的验证错误异常&#125;&#125; BindingResult是Spring MVC验证DTO后的一个结果集，可以参考spring 官方文档检查参数后，可以抛出一个“带验证码的验证错误异常”，具体异常设计可以参考如何优雅的设计java异常 拥抱lombok上边的DTO代码，已经让我看的很累了，我相信读者也是一样，看到那么多的Getter和Setter方法，太烦躁了，那时候有什么方法可以简化这些呢。请拥抱lombok,它会帮助我们解决一些让我们很烦躁的问题去掉Setter和Getter其实这个标题，我不太想说，因为网上太多，但是因为很多人告诉我，他们根本就不知道lombok的存在，所以为了让读者更好的学习，我愿意写这样一个例子： 1234567891011121314151617181920212223242526272829303132333435@Setter@Getterpublic class UserDTO &#123;@NotNullprivate String username;@NotNullprivate int age;public User convertToUser()&#123;UserDTOConvert userDTOConvert = new UserDTOConvert();User convert = userDTOConvert.convert(this);return convert;&#125;public UserDTO convertFor(User user)&#123;UserDTOConvert userDTOConvert = new UserDTOConvert();UserDTO convert = userDTOConvert.reverse().convert(user);return convert;&#125;private static class UserDTOConvert extends Converter&lt;UserDTO, User&gt; &#123;@Overrideprotected User doForward(UserDTO userDTO) &#123;User user = new User();BeanUtils.copyProperties(userDTO,user);return user;&#125;@Overrideprotected UserDTO doBackward(User user) &#123;throw new AssertionError("不支持逆向转化方法!");&#125;&#125;&#125; 看到了吧，烦人的Getter和Setter方法已经去掉了。但是上边的例子根本不足以体现lombok的强大。我希望写一些网上很难查到，或者很少人进行说明的lombok的使用以及在使用时程序语义上的说明。比如:@Data,@AllArgsConstructor,@NoArgsConstructor..这些我就不进行一一说明了，请大家自行查询资料. bean中的链式风格什么是链式风格？我来举个例子，看下面这个Student的bean: 123456789101112131415161718192021public class Student &#123;private String name;private int age;public String getName() &#123;return name;&#125;public Student setName(String name) &#123;this.name = name;return this;&#125;public int getAge() &#123;return age;&#125;public Student setAge(int age) &#123;return this;&#125;&#125; 仔细看一下set方法，这样的设置便是chain的style，调用的时候，可以这样使用: 123Student student = new Student().setAge(24).setName("zs"); 相信合理使用这样的链式代码，会更多的程序带来很好的可读性，那看一下如果使用lombok进行改善呢，请使用 @Accessors(chain = true),看如下代码: 1234567@Accessors(chain = true)@Setter@Getterpublic class Student &#123;private String name;private int age;&#125; 这样就完成了一个对于bean来讲很友好的链式操作。静态构造方法静态构造方法的语义和简化程度真的高于直接去new一个对象。比如new一个List对象，过去的使用是这样的: 1List&lt;String&gt; list = new ArrayList&lt;&gt;(); 看一下guava中的创建方式: 1List&lt;String&gt; list = Lists.newArrayList(); Lists命名是一种约定(俗话说：约定优于配置)，它是指Lists是List这个类的一个工具类，那么使用List的工具类去产生List，这样的语义是不是要比直接new一个子类来的更直接一些呢，答案是肯定的，再比如如果有一个工具类叫做Maps，那你是否想到了创建Map的方法呢： 1HashMap&lt;String, String&gt; objectObjectHashMap = Maps.newHashMap(); 好了，如果你理解了我说的语义，那么，你已经向成为java程序员更近了一步了。再回过头来看刚刚的Student，很多时候，我们去写Student这个bean的时候，他会有一些必输字段，比如Student中的name字段，一般处理的方式是将name字段包装成一个构造方法，只有传入name这样的构造方法，才能创建一个Student对象。接上上边的静态构造方法和必传参数的构造方法，使用lombok将更改成如下写法（@RequiredArgsConstructor 和 @NonNull）: 12345678@Accessors(chain = true)@Setter@Getter@RequiredArgsConstructor(staticName = "ofName")public class Student &#123;@NonNull private String name;private int age;&#125; 测试代码: 1Student student = Student.ofName("zs"); 这样构建出的bean语义是否要比直接new一个含参的构造方法(包含 name的构造方法)要好很多。当然，看过很多源码以后，我想相信将静态构造方法ofName换成of会先的更加简洁: 12345678@Accessors(chain = true)@Setter@Getter@RequiredArgsConstructor(staticName = "of")public class Student &#123;@NonNull private String name;private int age;&#125; 测试代码: 1Student student = Student.of("zs"); 当然他仍然是支持链式调用的: 1Student student = Student.of("zs").setAge(24); 这样来写代码，真的很简洁，并且可读性很强。 使用builderBuilder模式我不想再多解释了，读者可以看一下《Head First》(设计模式) 的建造者模式。今天其实要说的是一种变种的builder模式，那就是构建bean的builder模式，其实主要的思想是带着大家一起看一下lombok给我们带来了什么。看一下Student这个类的原始builder状态: 123456789101112131415161718192021222324252627282930313233343536373839404142434445public class Student &#123;private String name;private int age;public String getName() &#123;return name;&#125;public void setName(String name) &#123;this.name = name;&#125;public int getAge() &#123;return age;&#125;public void setAge(int age) &#123;this.age = age;&#125;public static Builder builder()&#123;return new Builder();&#125;public static class Builder&#123;private String name;private int age;public Builder name(String name)&#123;this.name = name;return this;&#125;public Builder age(int age)&#123;this.age = age;return this;&#125;public Student build()&#123;Student student = new Student();student.setAge(age);student.setName(name);return student;&#125;&#125;&#125; 调用方式: 1Student student = Student.builder().name("zs").age(24).build(); 这样的builder代码，让我是在恶心难受，于是我打算用lombok重构这段代码: 12345@Builderpublic class Student &#123;private String name;private int age;&#125; 调用方式: 1Student student = Student.builder().name("zs").age(24).build(); 代理模式正如我们所知的，在程序中调用rest接口是一个常见的行为动作，如果你和我一样使用过spring 的RestTemplate,我相信你会我和一样，对他抛出的非http状态码异常深恶痛绝。所以我们考虑将RestTemplate最为底层包装器进行包装器模式的设计: 123456789public abstract class FilterRestTemplate implements RestOperations &#123;protected volatile RestTemplate restTemplate;protected FilterRestTemplate(RestTemplate restTemplate)&#123;this.restTemplate = restTemplate;&#125;//实现RestOperations所有的接口&#125; 然后再由扩展类对FilterRestTemplate进行包装扩展: 123456789101112131415161718192021222324public class ExtractRestTemplate extends FilterRestTemplate &#123;private RestTemplate restTemplate;public ExtractRestTemplate(RestTemplate restTemplate) &#123;super(restTemplate);this.restTemplate = restTemplate;&#125;public &lt;T&gt; RestResponseDTO&lt;T&gt; postForEntityWithNoException(String url, Object request, Class&lt;T&gt; responseType, Object... uriVariables)throws RestClientException &#123;RestResponseDTO&lt;T&gt; restResponseDTO = new RestResponseDTO&lt;T&gt;();ResponseEntity&lt;T&gt; tResponseEntity;try &#123;tResponseEntity = restTemplate.postForEntity(url, request, responseType, uriVariables);restResponseDTO.setData(tResponseEntity.getBody());restResponseDTO.setMessage(tResponseEntity.getStatusCode().name());restResponseDTO.setStatusCode(tResponseEntity.getStatusCodeValue());&#125;catch (Exception e)&#123;restResponseDTO.setStatusCode(RestResponseDTO.UNKNOWN_ERROR);restResponseDTO.setMessage(e.getMessage());restResponseDTO.setData(null);&#125;return restResponseDTO;&#125;&#125; 包装器ExtractRestTemplate很完美的更改了异常抛出的行为，让程序更具有容错性。在这里我们不考虑ExtractRestTemplate完成的功能，让我们把焦点放在FilterRestTemplate上，“实现RestOperations所有的接口”,这个操作绝对不是一时半会可以写完的，当时在重构之前我几乎写了半个小时,如下: 1234567891011121314151617181920212223242526272829public abstract class FilterRestTemplate implements RestOperations &#123;protected volatile RestTemplate restTemplate;protected FilterRestTemplate(RestTemplate restTemplate) &#123;this.restTemplate = restTemplate;&#125;@Overridepublic &lt;T&gt; T getForObject(String url, Class&lt;T&gt; responseType, Object... uriVariables) throws RestClientException &#123;return restTemplate.getForObject(url,responseType,uriVariables);&#125;@Overridepublic &lt;T&gt; T getForObject(String url, Class&lt;T&gt; responseType, Map&lt;String, ?&gt; uriVariables) throws RestClientException &#123;return restTemplate.getForObject(url,responseType,uriVariables);&#125;@Overridepublic &lt;T&gt; T getForObject(URI url, Class&lt;T&gt; responseType) throws RestClientException &#123;return restTemplate.getForObject(url,responseType);&#125;@Overridepublic &lt;T&gt; ResponseEntity&lt;T&gt; getForEntity(String url, Class&lt;T&gt; responseType, Object... uriVariables) throws RestClientException &#123;return restTemplate.getForEntity(url,responseType,uriVariables);&#125;//其他实现代码略。。。&#125; 我相信你看了以上代码，你会和我一样觉得恶心反胃，后来我用lombok提供的代理注解优化了我的代码(@Delegate): 12345@AllArgsConstructorpublic abstract class FilterRestTemplate implements RestOperations &#123;@Delegateprotected volatile RestTemplate restTemplate;&#125; 这几行代码完全替代上述那些冗长的代码。是不是很简洁，做一个拥抱lombok的程序员吧。重构需求案例项目需求项目开发阶段，有一个关于下单发货的需求：如果今天下午3点前进行下单，那么发货时间是明天，如果今天下午3点后进行下单，那么发货时间是后天，如果被确定的时间是周日，那么在此时间上再加1天为发货时间。思考与重构我相信这个需求看似很简单，无论怎么写都可以完成。很多人可能看到这个需求，就动手开始写Calendar或Date进行计算，从而完成需求。而我给的建议是，仔细考虑如何写代码，然后再去写，不是说所有的时间操作都用Calendar或Date去解决，一定要看场景。对于时间的计算我们要考虑joda-time这种类似的成熟时间计算框架来写代码，它会让代码更加简洁和易读。请读者先考虑这个需求如何用java代码完成，或先写一个你觉得完成这个代码的思路，再来看我下边的代码，这样，你的收获会更多一些: 12345678910111213final DateTime DISTRIBUTION_TIME_SPLIT_TIME = new DateTime().withTime(15,0,0,0);private Date calculateDistributionTimeByOrderCreateTime(Date orderCreateTime)&#123;DateTime orderCreateDateTime = new DateTime(orderCreateTime);Date tomorrow = orderCreateDateTime.plusDays(1).toDate();Date theDayAfterTomorrow = orderCreateDateTime.plusDays(2).toDate();return orderCreateDateTime.isAfter(DISTRIBUTION_TIME_SPLIT_TIME) ? wrapDistributionTime(theDayAfterTomorrow) : wrapDistributionTime(tomorrow);&#125;private Date wrapDistributionTime(Date distributionTime)&#123;DateTime currentDistributionDateTime = new DateTime(distributionTime);DateTime plusOneDay = currentDistributionDateTime.plusDays(1);boolean isSunday = (DateTimeConstants.SUNDAY == currentDistributionDateTime.getDayOfWeek());return isSunday ? plusOneDay.toDate() : currentDistributionDateTime.toDate() ;&#125; 读这段代码的时候，你会发现，我将判断和有可能出现的不同结果都当做一个变量，最终做一个三目运算符的方式进行返回，这样的优雅和可读性显而易见，当然这样的代码不是一蹴而就的，我优化了3遍产生的以上代码。读者可根据自己的代码和我写的代码进行对比。 提高方法如果你做了3年+的程序员，我相信像如上这样的需求，你很轻松就能完成，但是如果你想做一个会写java的程序员，就好好的思考和重构代码吧。写代码就如同写字一样，同样的字，大家都会写，但是写出来是否好看就不一定了。如果想把程序写好，就要不断的思考和重构，敢于尝试，敢于创新，不要因循守旧，一定要做一个优秀的java程序员。提高代码水平最好的方法就是有条理的重构！(注意：是有条理的重构) 设计模式设计模式就是工具，而不是提现你是否是高水平程序员的一个指标。我经常会看到某一个程序员兴奋的大喊，哪个程序哪个点我用到了设计模式，写的多么多么优秀，多么多么好。我仔细去翻阅的时候，却发现有很多是过度设计的。 业务驱动技术 or 技术驱动业务业务驱动技术 or 技术驱动业务 ？ 其实这是一个一直在争论的话题，但是很多人不这么认为，我觉得就是大家不愿意承认罢了。我来和大家大概分析一下作为一个java程序员，我们应该如何判断自己所处于的位置.业务驱动技术：如果你所在的项目是一个收益很小或者甚至没有收益的项目，请不要搞其他创新的东西，不要驱动业务要如何如何做，而是要熟知业务现在的痛点是什么？如何才能帮助业务盈利或者让项目更好，更顺利的进行。技术驱动业务：如果你所在的项目是一个很牛的项目，比如淘宝这类的项目，我可以在满足业务需求的情况下，和业务沟通，使用什么样的技术能更好的帮助业务创造收益，比如说下单的时候要进队列，可能几分钟之后订单状态才能处理完成，但是会让用户有更流畅的体验，赚取更多的访问流量，那么我相信业务愿意被技术驱动，会同意订单的延迟问题，这样便是技术驱动业务。我相信大部分人还都处于业务驱动技术的方向吧。所以你既然不能驱动业务，那就请拥抱业务变化吧。 代码设计一直在做java后端的项目，经常会有一些变动，我相信大家也都遇到过。比如当我们写一段代码的时候，我们考虑将需求映射成代码的状态模式，突然有一天，状态模式里边又添加了很多行为变化的东西，这时候你就挠头了，你硬生生的将状态模式中添加过多行为和变化。慢慢的你会发现这些状态模式，其实更像是一簇算法，应该使用策略模式，这时你应该已经晕头转向了。说了这么多，我的意思是，只要你觉得合理，就请将状态模式改为策略模式吧，所有的模式并不是凭空想象出来的，都是基于重构。java编程中没有银弹，请拥抱业务变化，一直思考重构，你就有一个更好的代码设计!你真的优秀吗？真不好意思，我取了一个这么无聊的标题。国外流行一种编程方式，叫做结对编程，我相信国内很多公司都没有这么做，我就不在讲述结对编程带来的好处了，其实就是一边code review，一边互相提高的一个过程。既然做不到这个，那如何让自己活在自己的世界中不断提高呢？“平时开发的时候，做出的代码总认为是正确的，而且写法是完美的。”，我相信这是大部分人的心声，还回到刚刚的问题，如何在自己的世界中不断提高呢？答案就是: 1. 多看成熟框架的源码 2. 多回头看自己的代码 3. 勤于重构 你真的优秀吗？ 如果你每周都完成了学习源码，回头看自己代码，然后勤于重构，我认为你就真的很优秀了。即使也许你只是刚刚入门，但是一直坚持，你就是一个真的会写java代码的程序员了。 UML不想多讨论UML相关的知识，但是我觉得你如果真的会写java，请先学会表达自己，UML就是你说话的语言，做一名优秀的java程序员，请至少学会这两种UML图： 1. 类图 2. 时序图 clean code我认为保持代码的简洁和可读性是代码的最基本保证，如果有一天为了程序的效率而降低了这两点，我认为是可以谅解的，除此之外，没有任何理由可以让你任意挥霍你的代码。 1. 读者可以看一下Robert C. Martin出版的《Clean Code》（代码整洁之道） 这本书 2. 可以参考美团文章聊聊clean code 3. 也可以看一下阿里的Java编码规范 无论如何，请保持你的代码的整洁。linux 基础命令这点其实和会写java没有关系，但是linux很多时候确实承载运行java的容器，请学好linux的基础命令。 1. 参考鸟哥的《Linux私房菜》 总结java是一个大体系，今天讨论并未涉及框架和架构相关知识，只是讨论如何写好代码。本文从写java程序的小方面一直写到大方面，来阐述了如何才能写好java程序，并告诉读者们如何才能提高自身的编码水平。我希望看到这篇文章的各位都能做一个优秀的java程序员。坚持原创技术分享，您的支持将鼓励我继续创作！]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java攻城师成神之路]]></title>
    <url>%2F2017%2F02%2F04%2FJava%E6%94%BB%E5%9F%8E%E5%B8%88%E6%88%90%E7%A5%9E%E4%B9%8B%E8%B7%AF%2F</url>
    <content type="text"><![CDATA[针对本文，博主最近在写《成神之路系列文章》 ，分章分节介绍所有知识点。欢迎关注。 一、基础篇 1.1 JVM 1.1.1. Java内存模型，Java内存管理，Java堆和栈，垃圾回收http://www.jcp.org/en/jsr/detail?id=133http://ifeve.com/jmm-faq/ 1.1.2. 了解JVM各种参数及调优 1.1.3. 学习使用Java工具jps, jstack, jmap, jconsole, jinfo, jhat, javap, …http://kenai.com/projects/btracehttp://www.crashub.org/https://github.com/taobao/TProfilerhttps://github.com/CSUG/HouseMDhttp://wiki.cyclopsgroup.org/jmxtermhttps://github.com/jlusdy/TBJMap 1.1.4. 学习Java诊断工具http://www.eclipse.org/mat/http://visualvm.java.net/oqlhelp.html 1.1.5. 自己编写各种outofmemory，stackoverflow程序HeapOutOfMemoryYoung OutOfMemoryMethodArea OutOfMemoryConstantPool OutOfMemoryDirectMemory OutOfMemoryStack OutOfMemory Stack OverFlow 1.1.6. 使用工具尝试解决以下问题，并写下总结当一个Java程序响应很慢时如何查找问题 当一个Java程序频繁FullGC时如何解决问题，如何查看垃圾回收日志 当一个Java应用发生OutOfMemory时该如何解决，年轻代、年老代、永久代解决办法不同，导致原因也不同 1.1.7. 参考资料http://docs.oracle.com/javase/specs/jvms/se7/html/http://www.cs.umd.edu/~pugh/java/memoryModel/http://gee.cs.oswego.edu/dl/jmm/cookbook.htmlhttp://www.guru99.com/java-virtual-machine-jvm.html 1.2. Java基础知识 1.2.1. 阅读源代码java.lang.String java.lang.Integerjava.lang.Long java.lang.Enum java.math.BigDecimal java.lang.ThreadLocal java.lang.ClassLoader &amp; java.net.URLClassLoader java.util.ArrayList &amp; java.util.LinkedList java.util.HashMap &amp; java.util.LinkedHashMap &amp; java.util.TreeMap java.util.HashSet &amp; java.util.LinkedHashSet &amp; java.util.TreeSet 1.2.2. 熟悉Java中各种变量类型 1.2.3. 熟悉Java String的使用，熟悉String的各种函数 1.2.4. 熟悉Java中各种关键字 1.2.5. 学会使用List，Map，Stack，Queue，Set上述数据结构的遍历 上述数据结构的使用场景 Java实现对Array/List排序 java.uti.Arrays.sort() java.util.Collections.sort() Java实现对List去重 Java实现对List去重，并且需要保留数据原始的出现顺序 Java实现最近最少使用cache，用LinkedHashMap 1.2.6. Java IO&amp;Java NIO，并学会使用java.io. java.nio. nio和reactor设计模式 文件编码，字符集 1.2.7. Java反射与javassist反射与工厂模式 java.lang.reflect.* 1.2.8. Java序列化java.io. Serializable 什么是序列化，为什么序列化 序列化与单例模式 google序列化protobuf 1.2.9. 虚引用，弱引用，软引用java.lang.ref.* 实验这些引用的回收 1.2.10. 熟悉Java系统属性java.util.Properties 1.2.11. 熟悉Annotation用法java.lang.annotation.* 1.2.12. JMSjavax.jms.* 1.2.13. JMXjava.lang.management. javax.management. 1.2.14. 泛型和继承，泛型和擦除 1.2.15. 自动拆箱装箱与字节码 1.2.16. 实现Callback 1.2.17. java.lang.Void类使用 1.2.18. Java Agent，premain函数java.lang.instrument 1.2.19. 单元测试Junit，http://junit.org/Jmockit，https://code.google.com/p/jmockit/djUnit，http://works.dgic.co.jp/djunit/ 1.2.20. Java实现通过正则表达式提取一段文本中的电子邮件，并将@替换为#输出java.lang.util.regex.* 1.2.21. 学习使用常用的Java工具库commons.lang, commons.*… guava-libraries netty 1.2.22. 什么是API&amp;SPIhttp://en.wikipedia.org/wiki/Application_programming_interfacehttp://en.wikipedia.org/wiki/Service_provider_interface 1.2.23. 参考资料JDK src.zip 源代码http://openjdk.java.net/http://commons.apache.org/https://code.google.com/p/guava-libraries/http://netty.io/http://stackoverflow.com/questions/2954372/difference-between-spi-and-apihttp://stackoverflow.com/questions/11404230/how-to-implement-the-api-spi-pattern-in-java 1.3. Java并发编程 1.3.1. 阅读源代码，并学会使用java.lang.Thread java.lang.Runnable java.util.concurrent.Callable java.util.concurrent.locks.ReentrantLock java.util.concurrent.locks.ReentrantReadWriteLock java.util.concurrent.atomic.Atomic* java.util.concurrent.Semaphore java.util.concurrent.CountDownLatch java.util.concurrent.CyclicBarrier java.util.concurrent.ConcurrentHashMap java.util.concurrent.Executors 1.3.2. 学习使用线程池，自己设计线程池需要注意什么 1.3.3. 锁什么是锁，锁的种类有哪些，每种锁有什么特点，适用场景是什么 在并发编程中锁的意义是什么 1.3.4. synchronized的作用是什么，synchronized和lock 1.3.5. sleep和wait 1.3.6. wait和notify 1.3.7. 写一个死锁的程序 1.3.8. 什么是守护线程，守护线程和非守护线程的区别以及用法 1.3.9. volatile关键字的理解C++ volatile关键字和Java volatile关键字 happens-before语义 编译器指令重排和CPU指令重排http://en.wikipedia.org/wiki/Memory_orderinghttp://en.wikipedia.org/wiki/Volatile_variablehttp://preshing.com/20130702/the-happens-before-relation/ 1.3.10. 以下代码是不是线程安全？为什么？如果为count加上volatile修饰是否能够做到线程安全？你觉得该怎么做是线程安全的？publicclassSample{privatestaticint count =0;publicstaticvoid increment(){count++;}} 1.3.11. 解释一下下面两段代码的差别// 代码1publicclassSample{privatestaticint count =0;synchronizedpublicstaticvoid increment(){count++;}}// 代码2publicclassSample{privatestaticAtomicInteger count =newAtomicInteger(0);publicstaticvoid increment(){count.getAndIncrement();}} 1.3.12. 参考资料http://book.douban.com/subject/10484692/http://www.intel.com/content/www/us/en/processors/architectures-software-developer-manuals.html二、 进阶篇 2.1. Java底层知识 2.1.1. 学习了解字节码、class文件格式http://en.wikipedia.org/wiki/Java_class_filehttp://en.wikipedia.org/wiki/Java_bytecodehttp://en.wikipedia.org/wiki/Java_bytecode_instruction_listingshttp://www.csg.ci.i.u-tokyo.ac.jp/~chiba/javassist/http://asm.ow2.org/ 2.1.2. 写一个程序要求实现javap的功能（手工完成，不借助ASM等工具）如Java源代码：publicstaticvoid main(String[] args){int i =0;i +=1;i *=1;System.out.println(i);}编译后读取class文件输出以下代码：publicstaticvoid main(java.lang.String[]);Code:Stack=2,Locals=2,Args_size=10: iconst_01: istore_12: iinc 1,15: iload_16: iconst_17: imul8: istore_19: getstatic #2; //Field java/lang/System.out:Ljava/io/PrintStream;12: iload_113: invokevirtual #3; //Method java/io/PrintStream.println:(I)V16:returnLineNumberTable:line 4:0line 5:2line 6:5line 7:9line 8:16 2.1.3. CPU缓存，L1，L2，L3和伪共享http://duartes.org/gustavo/blog/post/intel-cpu-caches/http://mechanical-sympathy.blogspot.com/2011/07/false-sharing.html 2.1.4. 什么是尾递归 2.1.5. 熟悉位运算用位运算实现加、减、乘、除、取余 2.1.6. 参考资料http://book.douban.com/subject/1138768/http://book.douban.com/subject/6522893/http://en.wikipedia.org/wiki/Java_class_filehttp://en.wikipedia.org/wiki/Java_bytecodehttp://en.wikipedia.org/wiki/Java_bytecode_instruction_listings 2.2. 设计模式 2.2.1. 实现AOPCGLIB和InvocationHandler的区别 http://cglib.sourceforge.net/动态代理模式 Javassist实现AOP http://www.csg.ci.i.u-tokyo.ac.jp/~chiba/javassist/ASM实现AOP http://asm.ow2.org/ 2.2.2. 使用模板方法设计模式和策略设计模式实现IOC2.2.3. 不用synchronized和lock，实现线程安全的单例模式2.2.4. nio和reactor设计模式2.2.5. 参考资料http://asm.ow2.org/http://cglib.sourceforge.net/http://www.javassist.org/ 2.3. 网络编程知识 2.3.1. Java RMI，Socket，HttpClient 2.3.2. 用Java写一个简单的静态文件的HTTP服务器实现客户端缓存功能，支持返回304 实现可并发下载一个文件 使用线程池处理客户端请求 使用nio处理客户端请求 支持简单的rewrite规则 上述功能在实现的时候需要满足“开闭原则” 2.3.3. 了解nginx和apache服务器的特性并搭建一个对应的服务器http://nginx.org/http://httpd.apache.org/ 2.3.4. 用Java实现FTP、SMTP协议 2.3.5. 什么是CDN？如果实现？DNS起到什么作用？搭建一个DNS服务器 搭建一个 Squid 或 Apache Traffic Server 服务器 http://www.squid-cache.org/ http://trafficserver.apache.org/ http://en.wikipedia.org/wiki/Domain_Name_System 2.3.6. 参考资料http://www.ietf.org/rfc/rfc2616.txthttp://tools.ietf.org/rfc/rfc5321.txthttp://en.wikipedia.org/wiki/Open/closed_principle 2.4. 框架知识spring，spring mvc，阅读主要源码 ibatis，阅读主要源码 用spring和ibatis搭建java server 2.5. 应用服务器知识熟悉使用jboss，https://www.jboss.org/overview/ 熟悉使用tomcat，http://tomcat.apache.org/ 熟悉使用jetty，http://www.eclipse.org/jetty/三、 高级篇 3.1. 编译原理知识 3.1.1. 用Java实现以下表达式解析并返回结果（语法和Oracle中的select sysdate-1 from dual类似）sysdatesysdate -1sysdate -1/24sysdate -1/(12*2) 3.1.2. 实现对一个List通过DSL筛选QList&lt;Map&lt;String,Object&gt;&gt; mapList =newQList&lt;Map&lt;String,Object&gt;&gt;;mapList.add({“name”:”hatter test”});mapList.add({“id”:-1,”name”:”hatter test”});mapList.add({“id”:0,”name”:”hatter test”});mapList.add({“id”:1,”name”:”test test”});mapList.add({“id”:2,”name”:”hatter test”});mapList.add({“id”:3,”name”:”test hatter”});mapList.query(“id is not null and id &gt; 0 and name like ‘%hatter%’”);要求返回列表中匹配的对象，即最后两个对象； 3.1.3. 用Java实现以下程序（语法和变量作用域处理都和JavaScript类似）：代码：var a =1;var b =2;var c =function(){var a =3;println(a);println(b);};c();println(a);println(b);输出：3212 3.1.4. 参考资料http://en.wikipedia.org/wiki/Abstract_syntax_tree https://javacc.java.net/ http://www.antlr.org/ 3.2. 操作系统知识Ubuntu Centos 使用linux，熟悉shell脚本 3.3. 数据存储知识 3.3.1. 关系型数据库MySQL 如何看执行计划 如何搭建MySQL主备 binlog是什么 Derby，H2，PostgreSQL SQLite 3.3.2. NoSQLCache Redis Memcached Leveldb Bigtable HBase Cassandra Mongodb 图数据库 neo4j 3.3.3. 参考资料http://db-engines.com/en/rankinghttp://redis.io/https://code.google.com/p/leveldb/http://hbase.apache.org/http://cassandra.apache.org/http://www.mongodb.org/http://www.neo4j.org/ 3.4. 大数据知识 3.4.1. Zookeeper，在linux上部署zk 3.4.2. Solr，Lucene，ElasticSearch在linux上部署solr，solrcloud，，新增、删除、查询索引 3.4.3. Storm，流式计算，了解Spark，S4在linux上部署storm，用zookeeper做协调，运行storm hello world，local和remote模式运行调试storm topology。 3.4.4. Hadoop，离线计算Hdfs：部署NameNode，SecondaryNameNode，DataNode，上传文件、打开文件、更改文件、删除文件MapReduce：部署JobTracker，TaskTracker，编写mr jobHive：部署hive，书写hive sql，得到结果Presto：类hive，不过比hive快，非常值得学习 3.4.5. 分布式日志收集flume，kafka，logstash 3.4.6. 数据挖掘，mahout 3.4.7. 参考资料http://zookeeper.apache.org/https://lucene.apache.org/solr/https://github.com/nathanmarz/storm/wikihttp://hadoop.apache.org/http://prestodb.io/http://flume.apache.org/http://logstash.net/http://kafka.apache.org/http://mahout.apache.org/ 3.5. 网络安全知识 3.5.1. 什么是DES、AES 3.5.2. 什么是RSA、DSA 3.5.3. 什么是MD5，SHA1 3.5.4. 什么是SSL、TLS，为什么HTTPS相对比较安全 3.5.5. 什么是中间人攻击、如果避免中间人攻击 3.5.6. 什么是DOS、DDOS、CC攻击 3.5.7. 什么是CSRF攻击 3.5.8. 什么是CSS攻击 3.5.9. 什么是SQL注入攻击 3.5.10. 什么是Hash碰撞拒绝服务攻击 3.5.11. 了解并学习下面几种增强安全的技术http://www.openauthentication.org/HOTP http://www.ietf.org/rfc/rfc4226.txtTOTP http://tools.ietf.org/rfc/rfc6238.txtOCRA http://tools.ietf.org/rfc/rfc6287.txthttp://en.wikipedia.org/wiki/Salt_(cryptography) 3.5.12. 用openssl签一个证书部署到apache或nginx 3.5.13. 参考资料http://en.wikipedia.org/wiki/Cryptographic_hash_functionhttp://en.wikipedia.org/wiki/Block_cipherhttp://en.wikipedia.org/wiki/Public-key_cryptographyhttp://en.wikipedia.org/wiki/Transport_Layer_Securityhttp://www.openssl.org/https://code.google.com/p/google-authenticator/四、 扩展篇 4.1. 相关知识 4.1.1. 云计算，分布式，高可用，可扩展 4.1.2. 虚拟化https://linuxcontainers.org/http://www.linux-kvm.org/page/Main_Pagehttp://www.xenproject.org/https://www.docker.io/ 4.1.3. 监控http://www.nagios.org/http://ganglia.info/ 4.1.4. 负载均衡http://www.linuxvirtualserver.org/ 4.1.5. 学习使用githttps://github.com/https://git.oschina.net/ 4.1.6. 学习使用mavenhttp://maven.apache.org/ 4.1.7. 学习使用gradlehttp://www.gradle.org/ 4.1.8. 学习一个小语种语言Groovy Scala LISP, Common LISP, Schema, Clojure R Julia Lua Ruby 4.1.9. 尝试了解编码的本质了解以下概念 ASCII, ISO-8859-1 GB2312, GBK, GB18030 Unicode, UTF-8 不使用 String.getBytes() 等其他工具类/函数完成下面功能publicstaticvoid main(String[] args)throwsIOException{String str =”Hello, 我们是中国人。”;byte[] utf8Bytes = toUTF8Bytes(str);FileOutputStream fos =newFileOutputStream(“f.txt”);fos.write(utf8Bytes);fos.close();}publicstaticbyte[] toUTF8Bytes(String str){returnnull;// TODO}想一下上面的程序能不能写一个转GBK的？ 写个程序自动判断一个文件是哪种编码 4.1.10. 尝试了解时间的本质时区 &amp; 冬令时、夏令时 http://en.wikipedia.org/wiki/Time_zone ftp://ftp.iana.org/tz/data/asia http://zh.wikipedia.org/wiki/%E4%B8%AD%E5%9C%8B%E6%99%82%E5%8D%80闰年 http://en.wikipedia.org/wiki/Leap_year闰秒 ftp://ftp.iana.org/tz/data/leapsecondsSystem.currentTimeMillis() 返回的时间是什么 4.1.11. 参考资料http://git-scm.com/http://en.wikipedia.org/wiki/UTF-8http://www.iana.org/time-zones 4.2. 扩展学习 4.2.1. JavaScript知识 4.2.1.1. 什么是prototype修改代码，使程序输出“1 3 5”： http://jsfiddle.net/Ts7Fk/ 4.2.1.2. 什么是闭包看一下这段代码，并解释一下为什么按Button1时没有alert出“This is button: 1”，如何修改： http://jsfiddle.net/FDPj3/1/ 4.2.1.3. 了解并学习一个JS框架jQuery ExtJS ArgularJS 4.2.1.4. 写一个Greasemonkey插件http://en.wikipedia.org/wiki/Greasemonkey 4.2.1.5. 学习node.jshttp://nodejs.org/ 4.2.2. 学习html5ArgularJS，https://docs.angularjs.org/api 4.2.3. 参考资料http://www.ecmascript.org/http://jsfiddle.net/http://jsbin.com/http://runjs.cn/http://userscripts.org/五、 推荐书籍《深入Java虚拟机》《深入理解Java虚拟机》《Effective Java》《七周七语言》《七周七数据》《Hadoop技术内幕》《Hbase In Action》《Mahout In Action》《这就是搜索引擎》《Solr In Action》《深入分析Java Web技术内幕》《大型网站技术架构》《高性能MySQL》《算法导论》《计算机程序设计艺术》《代码大全》《JavaScript权威指南》]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式接口幂等性设计]]></title>
    <url>%2F2016%2F12%2F19%2F%E5%88%86%E5%B8%83%E5%BC%8F%E6%8E%A5%E5%8F%A3%E5%B9%82%E7%AD%89%E6%80%A7%E8%AE%BE%E8%AE%A1%2F</url>
    <content type="text"><![CDATA[关于分布式系统中接口幂等性设计的介绍 一、幂等性定义HTTP/1.1规范中幂等性的定义是：Methods can also have the property of “idempotence” in that (aside from error or expiration issues) the side-effects of N &gt; 0 identical requests is the same as for a single request.从定义上看，HTTP方法的幂等性是指一次和多次请求某一个资源应该具有同样的副作用。更多介绍请阅读理解HTTP幂等性 二、接口中幂等性设计查询操作查询一次和查询多次，在数据不变的情况下，查询结果是一样的，select是天然的幂等操作。 删除操作删除操作也是幂等的，删除一次和多次删除都是把数据删除。(注意可能返回结果不一样，删除的数据不存在，返回0，删除的数据多条，返回结果多个)。 唯一索引，防止新增脏数据比如：支付宝的资金账户，支付宝也有用户账户，每个用户只能有一个资金账户，怎么防止给用户创建资金账户多个，那么给资金账户表中的用户ID加唯一索引，所以一个用户新增成功一个资金账户记录。 token机制，防止页面重复提交业务要求：页面的数据只能被点击提交一次发生原因：由于重复点击或者网络重发，或者nginx重发等情况会导致数据被重复提交解决办法：集群环境：采用token加redis（redis单线程的，处理需要排队）单JVM环境：采用token加redis或token加jvm内存处理流程： 数据提交前要向服务的申请token，token放到redis或jvm内存，token有效时间 提交后后台校验token，同时删除token，生成新的token返回token特点：要申请，一次有效性，可以限流注意：redis要用删除操作来判断token，删除成功代表token校验通过，如果用select+delete来校验token，存在并发问题，不建议使用 悲观锁获取数据的时候加锁获取 1select * from table_xxx where id='xxx' for update; 注意：id字段一定是主键或者唯一索引，不然是锁表，会死人的悲观锁使用时一般伴随事务一起使用，数据锁定时间可能会很长，根据实际情况选用 乐观锁乐观锁只是在更新数据那一刻锁表，其他时间不锁表，所以相对于悲观锁，效率更高。 乐观锁的实现方式多种多样可以通过version或者其他状态条件： 通过版本号实现 1update table_xxx set name=#name#,version=version+1 where version=#version# 通过条件限制 1update table_xxx set avai_amount=avai_amount-#subAmount# where avai_amount-#subAmount# &gt;= 0 要求：quality-#subQuality# &gt;= ，这个情景适合不用版本号，只更新是做数据安全校验，适合库存模型，扣份额和回滚份额，性能更高 注意：乐观锁的更新操作，最好用主键或者唯一索引来更新,这样是行锁，否则更新时会锁表，上面两个sql改成下面的两个更好 12update table_xxx set name=#name#,version=version+1 where id=#id# and version=#version# update table_xxx set avai_amount=avai_amount-#subAmount# where id=#id# and avai_amount-#subAmount# &gt;= 0 分布式锁还是拿插入数据的例子，如果是分布是系统，构建全局唯一索引比较困难，例如唯一性的字段没法确定，这时候可以引入分布式锁，通过第三方的系统(redis或zookeeper)，在业务系统插入数据或者更新数据，获取分布式锁，然后做操作，之后释放锁，这样其实是把多线程并发的锁的思路，引入多多个系统，也就是分布式系统中得解决思路。 要点：某个长流程处理过程要求不能并发执行，可以在流程执行之前根据某个标志(用户ID+后缀等)获取分布式锁，其他流程执行时获取锁就会失败，也就是同一时间该流程只能有一个能执行成功，执行完成后，释放分布式锁(分布式锁要第三方系统提供) select + insert并发不高的后台系统，或者一些任务JOB，为了支持幂等，支持重复执行，简单的处理方法是，先查询下一些关键数据，判断是否已经执行过，在进行业务处理，就可以了注意：核心高并发流程不要用这种方法 状态机幂等在设计单据相关的业务，或者是任务相关的业务，肯定会涉及到状态机(状态变更图)，就是业务单据上面有个状态，状态在不同的情况下会发生变更，一般情况下存在有限状态机，这时候，如果状态机已经处于下一个状态，这时候来了一个上一个状态的变更，理论上是不能够变更的，这样的话，保证了有限状态机的幂等。 注意：订单等单据类业务，存在很长的状态流转，一定要深刻理解状态机，对业务系统设计能力提高有很大帮助 对外提供接口的api如何保证幂等如银联提供的付款接口：需要接入商户提交付款请求时附带：source来源，seq序列号source+seq在数据库里面做唯一索引，防止多次付款，(并发时，只能处理一个请求) 重点：对外提供接口为了支持幂等调用，接口有两个字段必须传，一个是来源source，一个是来源方序列号seq，这个两个字段在提供方系统里面做联合唯一索引，这样当第三方调用时，先在本方系统里面查询一下，是否已经处理过，返回相应处理结果；没有处理过，进行相应处理，返回结果。注意，为了幂等友好，一定要先查询一下，是否处理过该笔业务，不查询直接插入业务系统，会报错，但实际已经处理了。 参考阅读 高并发的核心技术-幂等性实现方案]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>幂等性</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTTPS原理]]></title>
    <url>%2F2016%2F12%2F01%2FHTTPS%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[为了保证这些隐私数据能加密传输，网景公司设计了SSL（Secure Sockets Layer）协议用于对HTTP协议传输的数据进行加密，从而就诞生了HTTPS。 一、HTTPS原理为了保证这些隐私数据能加密传输，网景公司设计了SSL（Secure Sockets Layer）协议用于对HTTP协议传输的数据进行加密，从而就诞生了HTTPS。HTTPS在传输数据之前需要客户端（浏览器）与服务端（网站）之间进行一次握手，在握手过程中将确立双方加密传输数据的密码信息。TLS/SSL协议不仅仅是一套加密传输的协议，更是一件经过艺术家精心设计的艺术品，TLS/SSL中使用了非对称加密，对称加密以及HASH算法。握手过程的简单描述如下：1.浏览器将自己支持的一套加密规则发送给网站。2.网站从中选出一组加密算法与HASH算法，并将自己的身份信息以证书的形式发回给浏览器。证书里面包含了网站地址，加密公钥，以及证书的颁发机构等信息。3.获得网站证书之后浏览器要做以下工作：a) 验证证书的合法性（颁发证书的机构是否合法，证书中包含的网站地址是否与正在访问的地址一致等），如果证书受信任，则浏览器栏里面会显示一个小锁头，否则会给出证书不受信的提示。b) 如果证书受信任，或者是用户接受了不受信的证书，浏览器会生成一串随机数的密码，并用证书中提供的公钥加密。c) 使用约定好的HASH计算握手消息，并使用生成的随机数对消息进行加密，最后将之前生成的所有信息发送给网站。4.网站接收浏览器发来的数据之后要做以下的操作：a) 使用自己的私钥将信息解密取出密码，使用密码解密浏览器发来的握手消息，并验证HASH是否与浏览器发来的一致。b) 使用密码加密一段握手消息，发送给浏览器。5.浏览器解密并计算握手消息的HASH，如果与服务端发来的HASH一致，此时握手过程结束，之后所有的通信数据将由之前浏览器生成的随机密码并利用对称加密算法进行加密。这里浏览器与网站互相发送加密的握手消息并验证，目的是为了保证双方都获得了一致的密码，并且可以正常的加密解密数据，为后续真正数据的传输做一次测试。另外，HTTPS一般使用的加密与HASH算法如下：非对称加密算法：RSA，DSA/DSS对称加密算法：AES，RC4，3DESHASH算法：MD5，SHA1，SHA256其中非对称加密算法用于在握手过程中加密生成的密码，对称加密算法用于对真正传输的数据进行加密，而HASH算法用于验证数据的完整性。由于浏览器生成的密码是整个数据加密的关键，因此在传输的时候使用了非对称加密算法对其加密。非对称加密算法会生成公钥和私钥，公钥只能用于加密数据，因此可以随意传输，而网站的私钥用于对数据进行解密，所以网站都会非常小心的保管自己的私钥，防止泄漏。 参考阅读 HTTPS那些事（一）HTTPS原理]]></content>
      <categories>
        <category>基础</category>
      </categories>
      <tags>
        <tag>Http</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[经纬度坐标转换]]></title>
    <url>%2F2016%2F11%2F24%2F%E7%BB%8F%E7%BA%AC%E5%BA%A6%E5%9D%90%E6%A0%87%E8%BD%AC%E6%8D%A2%2F</url>
    <content type="text"><![CDATA[利用Geotools工具转换经纬度坐标为需要的投影坐标。 名词扫盲EPSGEPSG：European Petroleum Survey Group (EPSG)， http://www.epsg.org/，它成立于1986年，并在2005年重组为OGP(Internation Association of Oil &amp; Gas Producers)，它负责维护并发布坐标参照系统的数据集参数，以及坐标转换描述，该数据集被广泛接受并使用，通过一个Web发布平台进行分发，同时提供了微软Acess数据库的存储文件，通过SQL 脚本文件，mySQL, Oracle 和PostgreSQL等数据库也可使用。目前已有的椭球体，投影坐标系等不同组合都对应着不同的ID号，这个号在EPSG中被称为EPSG code，它代表特定的椭球体、单位、地理坐标系或投影坐标系等信息。 SRIDSRID：OGC标准中的参数SRID，也是指的空间参考系统的ID，与EPSG一致；WMS 1.1.1以前用SRS参数（空间参考系）表示坐标系统，WMS1.3开始用CRS参数（坐标参考系统）来表示。 A Spatial Reference System Identifier(SRID) is a unique value used to unambiguously identify projected, unprojected, and local spatial coordinate system definitions. These coordinate systems form the heart of all GIS applications. Virtually all major spatial vendors have created their own SRID implementation or refer to those of an authority, such as the European Petroleum Survey Group (EPSG). (NOTE: As of 2005 the EPSG SRID values are now maintained by the International Association of Oil &amp; Gas Producers (OGP) Surveying &amp; Positioning Committee).以OGC请求为例：http://localhost/IS/WebServices/wms.ashx?map=World&amp;SERVICE=WMS&amp;REQUEST=GetMap&amp;LAYERS=&amp;STYLES=&amp;SRS=EPSG:4326&amp;BBOX=-3,44,10,53&amp;WIDTH=600&amp;HEIGHT=300&amp;FORMAT=image/gif&amp;BGCOLOR=&amp;VERSION=1.1.1SRS=EPSG:4326代表地理坐标系WGS1984 WKT空间参考系统的文字描述；无论是参考椭球、基准面、投影方式、坐标单位等，都有相应 的EPSG值表示。 举例：Beijing 1954地理坐标系，高斯–克吕格投影（横轴等角切圆柱投影）下面为投影相关信息：投影方式 Gauss_Kruger中央经线 75.000000原点纬线 0.000000标准纬线(1) 0.000000标准纬线(2) 0.000000水平偏移量 13500000.000000垂直偏移量 0.000000比例因子 1.000000方位角 0.000000第一点经线 0.000000第二点经线 0.000000地理坐标系 GCS_Beijing_1954大地参照系 D_Beijing_1954参考椭球体 Krasovsky_1940椭球长半轴 6378245.000000椭球扁率 0.0033523299本初子午线 0.000000 WKT形式表示该投影坐标系：PROJCS[“Gauss_Kruger”,GEOGCS[“GCS_Beijing_1954”, DATUM[“D_Beijing_1954”, SPHEROID[“Krasovsky_1940”,6378245.000000,298.299997264589]] ]PEIMEM[“Greenwich”,0]UNIT[“degree”,0.0174532925199433]//地理单位：0.0174532925199433代表与米之间的转换],PROJECTION[“Gauss_Kruger”],PARAMETER[“False_Easting”,13500000.000000],PARAMETER[“False_Northing”,0],PARAMETER[“Central_Meridian”,75.000000],PARAMETER[“Scale_Factor”,1.0],PARAMETER[“Latitude_Of_Origin”,0.0],UNIT[“Meter”,1.0]] ; GeoToolsGeoTools is an open source Java library that provides tools for geospatial data. 本文主要介绍利用GeoTools进行坐标系转换，将经纬度坐标转为投影坐标。maven引入依赖包 &lt;dependency&gt; &lt;groupId&gt;org.geotools&lt;/groupId&gt; &lt;artifactId&gt;gt-referencing&lt;/artifactId&gt; &lt;version&gt;16.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.geotools&lt;/groupId&gt; &lt;artifactId&gt;gt-epsg-wkt&lt;/artifactId&gt; &lt;version&gt;16.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.geotools&lt;/groupId&gt; &lt;artifactId&gt;gt-api&lt;/artifactId&gt; &lt;version&gt;16.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.geotools&lt;/groupId&gt; &lt;artifactId&gt;gt-opengis&lt;/artifactId&gt; &lt;version&gt;16.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.geotools&lt;/groupId&gt; &lt;artifactId&gt;gt-metadata&lt;/artifactId&gt; &lt;version&gt;16.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.vividsolutions&lt;/groupId&gt; &lt;artifactId&gt;jts&lt;/artifactId&gt; &lt;version&gt;1.13&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.googlecode.efficient-java-matrix-library&lt;/groupId&gt; &lt;artifactId&gt;ejml&lt;/artifactId&gt; &lt;version&gt;0.25&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;eu.agrosense.apps&lt;/groupId&gt; &lt;artifactId&gt;launcher&lt;/artifactId&gt; &lt;version&gt;14.02-beta&lt;/version&gt; &lt;/dependency&gt; 核心代码 public static double[] convert(double lon, double lat) throws FactoryException, MismatchedDimensionException, TransformException { // 传入原始的经纬度坐标 Coordinate sourceCoord = new Coordinate(lon, lat); GeometryFactory geoFactory = new GeometryFactory(); Point sourcePoint = geoFactory.createPoint(sourceCoord); // 这里是以OGC WKT形式定义的是World Mercator投影，网页地图一般使用该投影 final String strWKTMercator = &quot;PROJCS[\&quot;World_Mercator\&quot;,&quot; + &quot;GEOGCS[\&quot;GCS_WGS_1984\&quot;,&quot; + &quot;DATUM[\&quot;WGS_1984\&quot;,&quot; + &quot;SPHEROID[\&quot;WGS_1984\&quot;,6378137,298.257223563]],&quot; + &quot;PRIMEM[\&quot;Greenwich\&quot;,0],&quot; + &quot;UNIT[\&quot;Degree\&quot;,0.017453292519943295]],&quot; + &quot;PROJECTION[\&quot;Mercator_1SP\&quot;],&quot; + &quot;PARAMETER[\&quot;False_Easting\&quot;,0],&quot; + &quot;PARAMETER[\&quot;False_Northing\&quot;,0],&quot; + &quot;PARAMETER[\&quot;Central_Meridian\&quot;,0],&quot; + &quot;PARAMETER[\&quot;latitude_of_origin\&quot;,0],&quot; + &quot;UNIT[\&quot;Meter\&quot;,1]]&quot;; CoordinateReferenceSystem mercatroCRS = CRS.parseWKT(strWKTMercator); // 做投影转换，将WCG84坐标转换成世界墨卡托投影转 MathTransform transform = CRS.findMathTransform(DefaultGeographicCRS.WGS84, mercatroCRS); Point targetPoint = (Point) JTS.transform(sourcePoint, transform); // 返回转换以后的X和Y坐标 double[] targetCoord = {targetPoint.getX(), targetPoint.getY()}; return targetCoord; } 上述方法中引用的是OGC WKT形式定义的world mercator投影，更多WKT可查看http://spatialreference.org/]]></content>
      <categories>
        <category>随记</category>
      </categories>
      <tags>
        <tag>坐标转换</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker Logging via EFK (Elasticsearch + Fluentd + Kibana) Stack with Docker Compose]]></title>
    <url>%2F2016%2F11%2F24%2FDocker%20Logging%20via%20EFK%2F</url>
    <content type="text"><![CDATA[This article explains how to collect Docker logs to EFK (Elasticsearch + Fluentd + Kibana) stack. The example uses Docker Compose for setting up multiple containers. Elasticsearch is an open source search engine known for its ease of use. Kibana is an open source Web UI that makes Elasticsearch user friendly for marketers, engineers and data scientists alike.By combining these three tools EFK (Elasticsearch + Fluentd + Kibana) we get a scalable, flexible, easy to use log collection and analytics pipeline. In this article, we will set up 4 containers, each includes: * Apache HTTP Server * Fluentd * Elasticsearch * Kibana All of httpd’s logs will be ingested into Elasticsearch + Kibana, via Fluentd.Table of Contents * Prerequisites: Docker * Step 0: prepare docker-compose.yml * Step 1: Prepare Fluentd image with your Config + Plugin * Step 2: Start Containers * Step 3: Generate httpd Access Logs * Step 4: Confirm Logs from Kibana * Conclusion * Learn More Prerequisites: DockerPlease download and install Docker / Docker Compose. Well, that’s it :) Docker Installation Step 0: prepare docker-compose.ymlFirst, please prepare docker-compose.yml for Docker Compose.Docker Compose is a tool for defining and running multi-container Docker applications.With the YAML file below, you can create and start all the services (in this case, Apache, Fluentd, Elasticsearch, Kibana) by one command. 12345678910111213141516171819202122232425262728293031323334353637version: '2'services:web:image: httpdports:- "80:80"links:- fluentdlogging:driver: "fluentd"options:fluentd-address: localhost:24224tag: httpd.accessfluentd:build: ./fluentdvolumes:- ./fluentd/conf:/fluentd/etclinks:- "elasticsearch"ports:- "24224:24224"- "24224:24224/udp"elasticsearch:image: elasticsearchexpose:- 9200ports:- "9200:9200"kibana:image: kibanalinks:- "elasticsearch"ports:- "5601:5601" logging section (check Docker Compose documentation) of web container specifies Docker Fluentd Logging Driver as a default container logging driver. All of the logs from web container will be automatically forwarded to host:port specified by fluentd-address. Step 1: Prepare Fluentd image with your Config + PluginThen, please prepare fluentd/Dockerfile with the following content, to use Fluentd’s official Docker image and additionally install Elasticsearch plugin. 123#fluentd/DockerfileFROM fluent/fluentd:v0.12-debianRUN ["gem", "install", "fluent-plugin-elasticsearch", "--no-rdoc", "--no-ri", "--version", "1.9.2"] Then, please prepare Fluentd’s configuration file fluentd/conf/fluent.conf. in_forward plugin is used for receive logs from Docker logging driver, and out_elasticsearch is for forwarding logs to Elasticsearch. 123456789101112131415161718192021222324#fluentd/conf/fluent.conf&lt;source&gt;@type forwardport 24224bind 0.0.0.0&lt;/source&gt;&lt;match *.**&gt;@type copy&lt;store&gt;@type elasticsearchhost elasticsearchport 9200logstash_format truelogstash_prefix fluentdlogstash_dateformat %Y%m%dinclude_tag_key truetype_name access_logtag_key @log_nameflush_interval 1s&lt;/store&gt;&lt;store&gt;@type stdout&lt;/store&gt;&lt;/match&gt; Step 2: Start ContainersLet’s start all of the containers, with just one command. 1$ docker-compose up You can check to see if 4 containers are running by docker ps command. 123456$ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES2d28323d77a3 httpd "httpd-foreground" About an hour ago Up 43 seconds 0.0.0.0:80-&gt;80/tcp dockercomposeefk_web_1a1b15a7210f6 dockercomposeefk_fluentd "/bin/sh -c 'exec ..." About an hour ago Up 45 seconds 5140/tcp, 0.0.0.0:24224-&gt;24224/tcp, 0.0.0.0:24224-&gt;24224/udp dockercomposeefk_fluentd_101e43b191cc1 kibana "/docker-entrypoin..." About an hour ago Up 45 seconds 0.0.0.0:5601-&gt;5601/tcp dockercomposeefk_kibana_1b7b439415898 elasticsearch "/docker-entrypoin..." About an hour ago Up 50 seconds 0.0.0.0:9200-&gt;9200/tcp, 9300/tcp dockercomposeefk_elasticsearch_1 Step 3: Generate httpd Access LogsLet’s access to httpd to generate some access logs. curl command is always your friend. 1234567891011$ repeat 10 curl http://localhost:80/&lt;html&gt;&lt;body&gt;&lt;h1&gt;It works!&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;&lt;html&gt;&lt;body&gt;&lt;h1&gt;It works!&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;&lt;html&gt;&lt;body&gt;&lt;h1&gt;It works!&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;&lt;html&gt;&lt;body&gt;&lt;h1&gt;It works!&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;&lt;html&gt;&lt;body&gt;&lt;h1&gt;It works!&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;&lt;html&gt;&lt;body&gt;&lt;h1&gt;It works!&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;&lt;html&gt;&lt;body&gt;&lt;h1&gt;It works!&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;&lt;html&gt;&lt;body&gt;&lt;h1&gt;It works!&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;&lt;html&gt;&lt;body&gt;&lt;h1&gt;It works!&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;&lt;html&gt;&lt;body&gt;&lt;h1&gt;It works!&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt; Step 4: Confirm Logs from KibanaPlease go to http://localhost:5601/ with your browser. Then, you need to set up the index name pattern for Kibana. Please specify fluentd-* to Index name or pattern and press Create button.Then, go to Discover tab to seek for the logs. As you can see, logs are properly collected into Elasticsearch + Kibana, via Fluentd. ConclusionThis article explains how to collect logs from Apache to EFK (Elasticsearch + Fluentd + Kibana). The example code is available in this repository. * https://github.com/kzk/docker-compose-efk Learn More Fluentd ArchitectureFluentd Get StartedDownloading Fluentd]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker日志</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[吭书吧]]></title>
    <url>%2F2016%2F10%2F14%2F%E6%88%91%E7%9A%84%E8%AF%BB%E4%B9%A6%2F</url>
    <content type="text"><![CDATA[书山有路勤为径 未读书单《Spring Boot实战》《JavaEE开发的颠覆者 Spring Boot实战 完整版》《Spring技术内幕：深入解析Spring架构与设计原理(第2版)》《Gradle User Guide》《JHipster》已读书单坚持每天读书，写下读书笔记。]]></content>
      <categories>
        <category>读书</category>
      </categories>
      <tags>
        <tag>读书</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu16安装rtx]]></title>
    <url>%2F2016%2F09%2F18%2FUbuntu16%E5%AE%89%E8%A3%85rtx%2F</url>
    <content type="text"><![CDATA[Ubuntu16.04系统中安装腾讯通RTX走过的那些坑 一、腾讯RTX下载 腾讯通RTX是（Real Time eXpert）是腾讯公司推出的企业级实时通信平台，致力于帮助企业提高运作效率、降低沟通成本、拓展商业机会，是一种高度可管理、低成本、易部署的IT平台。RTX集成了丰富的沟通方式，包括文本会话、语音/视频交流、手机短信、文件传输、IP电话、网络会议、以及应用程序共享、电子白板等远程协作方式。 链接至腾讯通官网下载地址下载rtx客户端，最新版本是2015版。 二、安装wine Wine (Wine Is Not an Emulator)[即Wine不是一个模拟器]是一个在Linux和UNIX之上的,Windows 3.x和 Windows APIs的实现。注意，Wine不是Windows模拟器，而是运用API转换技术实做出Linux对应到Windows相对应的函数来调用DLL以运行Windows程序。 因为rtx没有Linux版本，所以我们需要在Ubuntu中安装wine来解决打开exe文件的问题。利用PPA源进行快速安装wine 12sudo add-apt-repository ppa:wine/wine-buildssudo apt-get update 推荐安装winehq官方提供的最新版本wine 1sudo apt-get install wine-devel 安装wine下的window扩展包安装工具winetricks,在终端输入 123sudo wget http://winetricks.org/winetrickschmod +x winetrickssudo mv winetricks /usr/local/bin 安装rtx用到的相关扩展，在终端输入 1winetricks msxml3 gdiplus riched20 riched30 ie6 vcrun6 vcrun2005sp1 注意：如果相关扩展无法下载，可以去找其它下载源，手动解压到~/.cache/winetricks下。 ##安装RTX上述环境都准备好后，在终端输入 1wine rtxclient2015formal.exe ##中文方框乱码问题解决终端输入 winetricks corefonts下载字体字体1字体2解压上面的两个文件后，拷贝文泉驿字体，并导入注册表文件 12cp wqy-microhei/wqy-microhei.ttc ~/.wine/drive_c/windows/Fonts/.regedit font.reg 全部完成！ 参考阅读 字体问题解决 安装教程]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
</search>
